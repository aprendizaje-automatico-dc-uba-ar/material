{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aprendizaje-automatico-dc-uba-ar/material/blob/main/notebooks/notebook_09_redes_neuronales-published.ipynb)\n",
    "\n",
    "# Redes neuronales\n",
    "\n",
    "\n",
    "Vamos nuevamente a trabajar con los datos de `iris` para entrenar (y antes construir) una Red Neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_data():\n",
    "    dataset = load_iris()\n",
    "    X = dataset[\"data\"]\n",
    "    y = dataset[\"target\"]\n",
    "    y = LabelEncoder().fit_transform(y)\n",
    "    return np.array(X), np.array(y)\n",
    "X, y = get_data()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La propuesta es empezar por el esqueleto de las 2 clases que usaremos para esta tarea e ir implementado los métodos a medida que avancemos.\n",
    "\n",
    "Al final de este notebook se encuentran ambas clases completas. Pueden copiar el código desde allí mismo o implementarlo. La idea es que en cada avance podamos comprender la parte del proceso que estamos realizando, por lo cual se recomienda seguir la guia propuesta e ir completando sólo lo que es necesario para cada punto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Capa:\n",
    "    def __init__(self, neuronas):\n",
    "        self.neuronas = neuronas\n",
    "\n",
    "    def forward(self, inputs, weights, bias, activation):\n",
    "        \"\"\"\n",
    "        Forward Propagation de la capa\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def relu(self, inputs):\n",
    "        \"\"\"\n",
    "        ReLU: función de activación\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def softmax(self, inputs):\n",
    "        \"\"\"\n",
    "        Softmax: función de activación\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, dA_curr, W_curr, Z_curr, A_prev, activation):\n",
    "        \"\"\"\n",
    "        Backward Propagation de la capa\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def relu_derivative(self, dA, Z):\n",
    "        \"\"\"\n",
    "        ReLU: gradiente de ReLU\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedNeuronal:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.red = [] ## capas\n",
    "        self.arquitectura = [] ## mapeo de entradas -> salidas\n",
    "        self.pesos = [] ## W, b\n",
    "        self.memoria = [] ## Z, A\n",
    "        self.gradientes = [] ## dW, db\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "    def add(self, capa):\n",
    "        \"\"\"\n",
    "        Agregar capa a la red\n",
    "        \"\"\"\n",
    "        self.network.append(capa)\n",
    "            \n",
    "    def _compile(self, data):\n",
    "        \"\"\"\n",
    "        Inicializar la arquitectura\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _init_weights(self, data):\n",
    "        \"\"\"\n",
    "        Inicializar arquitectura y los pesos\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _forwardprop(self, data):\n",
    "        \"\"\"\n",
    "        Pasada forward completa por la red\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _backprop(self, predicted, actual):\n",
    "        \"\"\"\n",
    "        Pasada backward completa por la red\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "            \n",
    "    def _update(self):\n",
    "        \"\"\"\n",
    "        Actualizar el modelo --> lr * gradiente\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _get_accuracy(self, predicted, actual):\n",
    "        \"\"\"\n",
    "        Calcular accuracy después de cada iteración\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _calculate_loss(self, predicted, actual):\n",
    "        \"\"\"\n",
    "        Calcular cross-entropy loss después de cada iteración\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def train(self, X_train, y_train, epochs):\n",
    "        \"\"\"\n",
    "        Entrenar el modelo Stochastic Gradient Descent\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los items que se presentan a continuación tienen como objetivo explorar las clases que componen la red neuronal propuesta, comprender su arquitectura y funcionamiento.\n",
    "\n",
    "Nuevamente, lo ideal es no mirar todos los métodos hasta que llegue el momento de utilizarlos. \n",
    "\n",
    "1. Crear una Red Neuronal con 6 nodos en la primera capa, 8 en la segunda, 10 en la tercer y finalmente 3 en la última, utilizando los métodos `add()`, `_compile()` de la clase `RedNeuronal` y el constructor de la clase `Capa`.\n",
    "  \n",
    "    Imprimir la arquitectura del modelo y asegurarse de obtener:\n",
    "\n",
    "    ```\n",
    "    [{'input_dim': 4, 'output_dim': 6, 'activation': 'relu'},\n",
    "    {'input_dim': 6, 'output_dim': 8, 'activation': 'relu'},\n",
    "    {'input_dim': 8, 'output_dim': 10, 'activation': 'relu'},\n",
    "    {'input_dim': 10, 'output_dim': 3, 'activation': 'softmax'}]\n",
    "    ```\n",
    "\n",
    "    Dibujar la red en papel.\n",
    "\n",
    "1. Inicializar los pesos de la red del punto anterior (`_init_weights(datos)`) y verificar que los pesos tienen dimensión correcta:\n",
    "\n",
    "    ```\n",
    "    capa 0: w=(4, 6) - b=(1, 6)\n",
    "    capa 1: w=(6, 8) - b=(1, 8)\n",
    "    capa 2: w=(8, 10) - b=(1, 10)\n",
    "    capa 3: w=(10, 3) - b=(1, 3)\n",
    "    ```\n",
    "\n",
    "    Definir las matrices que se corresponden con las capas de manera que una pasada pueda ser interpretada como el producto de todas ellas. Recordar que en cada paso por cada capa estaremos computando por cada neurona de la capa siguiente:\n",
    "\n",
    "    $$Z = \\sum_{i=1}^{n} X_i \\times W_i + b$$\n",
    "\n",
    "1. Funciones de activación de una `Capa`:\n",
    "\n",
    "    1. Verificar que el funcionamiento de `ReLU` se corresponda con:\n",
    "\n",
    "        ```\n",
    "        if input > 0:\n",
    "            return input\n",
    "        else:\n",
    "            return 0\n",
    "        ``` \n",
    "\n",
    "    1. Verificar que el funcionamiento de `softmax` se corresponda con:\n",
    "\n",
    "        $$\\sigma(Z)_i = \\frac{e^{z_i}}{\\sum_{i=1}^{n} e^{z_j}}$$\n",
    "\n",
    "    **Nota**: para probar estos dos métodos puede ser util construir un vector de la siguiente manera: `np.array([[1.3, 5.1, -2.2, 0.7, 1.1]])` que genera un vector de tamaño (1,5).\n",
    "\n",
    "1. Avancemos con `_forwardprop(datos)`, si corremos la red inicializada con los datos:\n",
    "\n",
    "    1. ¿Qué nos tipo de objeto nos devuelve este método?\n",
    "\n",
    "    1. ¿Qué quiere decir cada uno de los valores?\n",
    "\n",
    "    1. La primera fila, que se correspondería con la primera observación del dataset, ¿qué resultados nos da?¿qué es más probable: 'setosa', 'versicolor' o 'virginica'?¿qué valor es el real?¿por qué?\n",
    "\n",
    "1. Arrancamos a propagar para atrás lo aprendido en la primera pasada. Esto lo realizaremos con el método `_backprop`.\n",
    "\n",
    "    1. ¿Cómo es la derivada de la función de activación `ReLU`?¿Su código es correcto?\n",
    "\n",
    "    1. ¿Cuál es la operación matemática que hace la función `backward` de la clase `Capa` en el caso de tener como activación a `relu`?\n",
    "\n",
    "    1. El método `_backprop` toma 2 parámetros: `predicted` y `actual`. ¿qué debemos pasarle en dicho lugar?\n",
    "\n",
    "        Si la respuesta no fue: en `predicted` le pasamos el resultado de `_forwardprop(...)` y en `actual` le pasamos `y`.... volver a pensarlo. ;-)\n",
    "\n",
    "    1. Verificar que los `gradientes` y los `pesos` para cada una de las capas tienen el mismo tamaño.\n",
    "\n",
    "1. Preparemos por último las funciones necesarias para el entrenamiento. Describir brevemente qué hacen las funciones:\n",
    "\n",
    "    - `_get_accuracy`\n",
    "    - `_calculate_loss`\n",
    "    - `_update`\n",
    "\n",
    "1. Incluyamos finalmente la función `train` y entrenemos una red con la arquitectura propuesta en el punto 1 por 200 epocas.\n",
    "\n",
    "    1. ¿Qué valores se imprimen?¿Qué es posible interpretar de ellos?\n",
    "\n",
    "    1. Graficar el _accuracy_ y la _loss_ que arroja el entramiento en función de las _epochs_. ¿Qué se puede concluir? Probablemente la señal sea ruidosa, por lo que se recomienda hacer un suavizado por ventanas deslizantes.\n",
    "\n",
    "1. Reimplementar la clase `RedNeuronal` utilizando PyTorch\n",
    "\n",
    "    Hasta ahora hemos construido nuestra propia red neuronal \"desde cero\", lo cual nos permitió comprender en profundidad cómo funciona cada componente: inicialización de pesos, funciones de activación, forward y backward propagation, cálculo de loss y accuracy, y actualización de pesos.\n",
    "\n",
    "    Sin embargo, en proyectos reales y más complejos, utilizamos frameworks como **PyTorch** que abstraen estas tareas, permitiéndonos enfocarnos más en el diseño de la arquitectura y el análisis de los resultados.  \n",
    "\n",
    "    **Objetivo de este inciso**: recrear la arquitectura y entrenamiento de nuestra red neuronal, pero usando herramientas provistas por PyTorch. Esto implica:\n",
    "\n",
    "    1. Implementar una clase `RedNeuronalTorch` que herede de `nn.Module` y contenga una red con la misma arquitectura:  \n",
    "    - Entrada de dimensión 4 (por las características del dataset Iris)\n",
    "    - Capas ocultas de 6, 8 y 10 nodos respectivamente\n",
    "    - Capa de salida con 3 nodos y activación `softmax`\n",
    "\n",
    "    2. Entrenar esta nueva red por 200 épocas utilizando:\n",
    "    - Función de pérdida: `nn.CrossEntropyLoss`\n",
    "    - Optimizador: `torch.optim.SGD`\n",
    "    - Tasa de aprendizaje: 0.01\n",
    "\n",
    "    3. Comparar los resultados obtenidos con los del entrenamiento anterior (implementación manual). Algunas preguntas a responder:\n",
    "    - ¿La convergencia es más rápida o más lenta?\n",
    "    - ¿Cómo se comporta la pérdida durante el entrenamiento?\n",
    "    - ¿Cuál implementación fue más fácil de modificar o extender?\n",
    "\n",
    "    4. Graficar la evolución de la **pérdida** y el **accuracy** durante las épocas para ambas implementaciones (manual y PyTorch), idealmente en la misma figura para facilitar la comparación. Podés aplicar una media móvil para suavizar la señal.\n",
    "\n",
    "    > 💡 **Sugerencia pedagógica**: antes de realizar este inciso, se recomienda repasar los notebooks `9a` y `9b`, donde se presentan una introducción a los tensores y al workflow de ML usando PyTorch.\n",
    "\n",
    "\n",
    "Crédito: este ejercicio se base en la propuesta de Joe Sasson publicada en [Towards Data Science](https://towardsdatascience.com/coding-a-neural-network-from-scratch-in-numpy-31f04e4d605)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código completo (Implementación con Numpy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Capa:\n",
    "    def __init__(self, neuronas):\n",
    "        self.neuronas = neuronas\n",
    "\n",
    "    def forward(self, inputs, weights, bias, activation):\n",
    "        \"\"\"\n",
    "        Forward Propagation de la capa\n",
    "        \"\"\"\n",
    "        Z_curr = np.dot(inputs, weights.T) + bias\n",
    "\n",
    "        if activation == 'relu':\n",
    "            A_curr = self.relu(inputs=Z_curr)\n",
    "        elif activation == 'softmax':\n",
    "            A_curr = self.softmax(inputs=Z_curr)\n",
    "\n",
    "        return A_curr, Z_curr\n",
    "\n",
    "    def relu(self, inputs):\n",
    "        \"\"\"\n",
    "        ReLU: función de activación\n",
    "        \"\"\"\n",
    "\n",
    "        return np.maximum(0, inputs)\n",
    "\n",
    "    def softmax(self, inputs):\n",
    "        \"\"\"\n",
    "        Softmax: función de activación\n",
    "        \"\"\"\n",
    "        exp_scores = np.exp(inputs)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        return probs\n",
    "         \n",
    "    def backward(self, dA_curr, W_curr, Z_curr, A_prev, activation):\n",
    "        \"\"\"\n",
    "        Backward Propagation de la capa\n",
    "        \"\"\"\n",
    "        if activation == 'softmax':\n",
    "            dW = np.dot(A_prev.T, dA_curr)\n",
    "            db = np.sum(dA_curr, axis=0, keepdims=True)\n",
    "            dA = np.dot(dA_curr, W_curr) \n",
    "        else:\n",
    "            dZ = self.relu_derivative(dA_curr, Z_curr)\n",
    "            dW = np.dot(A_prev.T, dZ)\n",
    "            db = np.sum(dZ, axis=0, keepdims=True)\n",
    "            dA = np.dot(dZ, W_curr)\n",
    "            \n",
    "        return dA, dW, db\n",
    "\n",
    "    def relu_derivative(self, dA, Z):\n",
    "        \"\"\"\n",
    "        ReLU: gradiente de ReLU\n",
    "        \"\"\"\n",
    "        dZ = np.array(dA, copy = True)\n",
    "        dZ[Z <= 0] = 0\n",
    "        return dZ\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedNeuronal:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.red = [] ## capas\n",
    "        self.arquitectura = [] ## mapeo de entradas -> salidas\n",
    "        self.pesos = [] ## W, b\n",
    "        self.memoria = [] ## Z, A\n",
    "        self.gradientes = [] ## dW, db\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "    def add(self, capa):\n",
    "        \"\"\"\n",
    "        Agregar capa a la red\n",
    "        \"\"\"\n",
    "        self.red.append(capa)\n",
    "            \n",
    "    def _compile(self, data):\n",
    "        \"\"\"\n",
    "        Inicializar la arquitectura\n",
    "        \"\"\"\n",
    "        for idx, _ in enumerate(self.red):\n",
    "            if idx == 0:\n",
    "                self.arquitectura.append({'input_dim': data.shape[1], \n",
    "                                        'output_dim': self.red[idx].neuronas,\n",
    "                                        'activation':'relu'})\n",
    "            elif idx > 0 and idx < len(self.red)-1:\n",
    "                self.arquitectura.append({'input_dim': self.red[idx-1].neuronas, \n",
    "                                        'output_dim': self.red[idx].neuronas,\n",
    "                                        'activation':'relu'})\n",
    "            else:\n",
    "                self.arquitectura.append({'input_dim': self.red[idx-1].neuronas, \n",
    "                                        'output_dim': self.red[idx].neuronas,\n",
    "                                        'activation':'softmax'})\n",
    "        return self\n",
    "\n",
    "    def _init_weights(self, data):\n",
    "        \"\"\"\n",
    "        Inicializar arquitectura y los pesos\n",
    "        \"\"\"\n",
    "        self._compile(data)\n",
    "\n",
    "        np.random.seed(99)\n",
    "\n",
    "        for i in range(len(self.arquitectura)):\n",
    "            self.pesos.append({\n",
    "                'W':np.random.uniform(low=-1, high=1, \n",
    "                        size=(self.arquitectura[i]['input_dim'],\n",
    "                            self.arquitectura[i]['output_dim']\n",
    "                            )),\n",
    "                'b':np.zeros((1, self.arquitectura[i]['output_dim']))})\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def _forwardprop(self, data):\n",
    "        \"\"\"\n",
    "        Pasada forward completa por la red\n",
    "        \"\"\"\n",
    "        A_curr = data\n",
    "\n",
    "        for i in range(len(self.pesos)):\n",
    "            A_prev = A_curr\n",
    "            A_curr, Z_curr = self.red[i].forward(inputs=A_prev, \n",
    "                                                    weights=self.pesos[i]['W'].T, \n",
    "                                                    bias=self.pesos[i]['b'], \n",
    "                                                    activation=self.arquitectura[i]['activation'])\n",
    "\n",
    "            self.memoria.append({'inputs':A_prev, 'Z':Z_curr})\n",
    "\n",
    "        return A_curr\n",
    "    \n",
    "    def _backprop(self, predicted, actual):\n",
    "        \"\"\"\n",
    "        Pasada backward completa por la red\n",
    "        \"\"\"\n",
    "        num_samples = len(actual)\n",
    "\n",
    "        ## compute the gradient on predictions\n",
    "        dscores = predicted\n",
    "        dscores[range(num_samples),actual] -= 1\n",
    "        dscores /= num_samples\n",
    "\n",
    "        dA_prev = dscores\n",
    "\n",
    "        for idx, layer in reversed(list(enumerate(self.red))):\n",
    "            dA_curr = dA_prev\n",
    "\n",
    "            A_prev = self.memoria[idx]['inputs']\n",
    "            Z_curr = self.memoria[idx]['Z']\n",
    "            W_curr = self.pesos[idx]['W']\n",
    "\n",
    "            activation = self.arquitectura[idx]['activation']\n",
    "\n",
    "            dA_prev, dW_curr, db_curr = layer.backward(dA_curr, W_curr.T, Z_curr, A_prev, activation)\n",
    "\n",
    "            self.gradientes.append({'dW':dW_curr, 'db':db_curr})\n",
    "\n",
    "        self.gradientes = list(reversed(self.gradientes))  # Reverse the gradients list\n",
    "\n",
    "    def _update(self):\n",
    "        \"\"\"\n",
    "        Actualizar el modelo --> lr * gradiente\n",
    "        \"\"\"\n",
    "        lr = self.lr\n",
    "        for idx, layer in enumerate(self.red):\n",
    "            self.pesos[idx]['W'] -= lr * self.gradientes[idx]['dW']\n",
    "            self.pesos[idx]['b'] -= lr * self.gradientes[idx]['db']\n",
    "\n",
    "    def _get_accuracy(self, predicted, actual):\n",
    "        \"\"\"\n",
    "        Calcular accuracy después de cada iteración\n",
    "        \"\"\"\n",
    "        return np.mean(np.argmax(predicted, axis=1)==actual)\n",
    "        \n",
    "    def _calculate_loss(self, predicted, actual):\n",
    "        \"\"\"\n",
    "        Calculate cross-entropy loss after each iteration\n",
    "        \"\"\"\n",
    "        samples = len(actual)\n",
    "\n",
    "        correct_logprobs = -np.log(predicted[range(samples),actual])\n",
    "        data_loss = np.sum(correct_logprobs)/samples\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def train(self, X_train, y_train, epochs):\n",
    "        \"\"\"\n",
    "        Entrenar el modelo Stochastic Gradient Descent\n",
    "        \"\"\"\n",
    "        self.loss = []\n",
    "        self.accuracy = []\n",
    "\n",
    "        self._init_weights(X_train)\n",
    "\n",
    "        for i in range(epochs):\n",
    "            yhat = self._forwardprop(X_train)\n",
    "            self.accuracy.append(self._get_accuracy(predicted=yhat, actual=y_train))\n",
    "            self.loss.append(self._calculate_loss(predicted=yhat, actual=y_train))\n",
    "\n",
    "            self._backprop(predicted=yhat, actual=y_train)\n",
    "\n",
    "            self._update()\n",
    "\n",
    "            if i % 20 == 0:\n",
    "                s = 'EPOCH: {}, ACCURACY: {}, LOSS: {}'.format(i, self.accuracy[-1], self.loss[-1])\n",
    "                print(s)\n",
    "\n",
    "        return (self.accuracy, self.loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "ocultar"
    ]
   },
   "outputs": [],
   "source": [
    "# Ocultar esta celda\n",
    "_dataset = load_iris()\n",
    "model = RedNeuronal()\n",
    "model.add(Capa(6))\n",
    "model.add(Capa(8))\n",
    "model.add(Capa(10))\n",
    "model.add(Capa(3))\n",
    "\n",
    "model._init_weights(X)\n",
    "print(model.arquitectura)\n",
    "\n",
    "for idx, c in enumerate(model.pesos):\n",
    "    print(f'capa {idx}: w={c[\"W\"].shape} - b={c[\"b\"].shape}')\n",
    "# print(len(model.pesos))\n",
    "\n",
    "\n",
    "out = model._forwardprop(X)\n",
    "print('SHAPE:', out.shape)\n",
    "print('Probabilties at idx 0:', out[0])\n",
    "print('Max', np.argmax(out[0]), _dataset[\"target_names\"][np.argmax(out[0])])\n",
    "print('Real', _dataset[\"target\"][0], _dataset[\"target_names\"][0])\n",
    "print('SUM:', sum(out[0]))\n",
    "\n",
    "model._backprop(predicted=out, actual=y)\n",
    "\n",
    "print(model.gradientes[0]['dW'].shape, model.pesos[3]['W'].shape)\n",
    "print(model.gradientes[1]['dW'].shape, model.pesos[2]['W'].shape)\n",
    "print(model.gradientes[2]['dW'].shape, model.pesos[1]['W'].shape)\n",
    "print(model.gradientes[3]['dW'].shape, model.pesos[0]['W'].shape)\n",
    "# c = Capa(4)\n",
    "# print(np.array([[1.3, 5.1, -2.2, 0.7, 1.1]]).shape)\n",
    "# print(c.relu(np.array([[1.3, 5.1, -2.2, 0.7, 1.1]])))\n",
    "# print(c.softmax(np.array([[1.3, 5.1, 2.2, 0.7, 1.1]])))\n",
    "\n",
    "model = RedNeuronal()\n",
    "model.add(Capa(6))\n",
    "model.add(Capa(8))\n",
    "model.add(Capa(10))\n",
    "model.add(Capa(3))\n",
    "epochs = 300\n",
    "accuracy, loss = model.train(X, y, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "ocultar"
    ]
   },
   "outputs": [],
   "source": [
    "# Esta celda se ocultará\n",
    "\n",
    "def sliding_window_smooth(data, window_size):\n",
    "    smoothed_data = []\n",
    "    half_window = window_size // 2\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        window_start = max(0, i - half_window)\n",
    "        window_end = min(len(data), i + half_window + 1)\n",
    "        window = data[window_start:window_end]\n",
    "        smoothed_data.append(np.mean(window))\n",
    "\n",
    "    return smoothed_data\n",
    "\n",
    "window_size = 10  # Tamaño de la ventana para suavizar\n",
    "smoothed_y = sliding_window_smooth(accuracy, window_size)\n",
    "\n",
    "plt.plot(range(epochs), accuracy)\n",
    "plt.plot(range(epochs), smoothed_y, label='Datos suavizados')\n",
    "\n",
    "plt.title('Accuracy de la red', fontsize=20)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(range(epochs), loss)\n",
    "plt.plot(range(epochs), sliding_window_smooth(loss, window_size))\n",
    "plt.title('Loss de la red', fontsize=20)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "ocultar"
    ]
   },
   "outputs": [],
   "source": [
    "# Esta celda se ocultará\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Cargar y preparar los datos\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train_t = torch.tensor(X, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# 2. Definir la clase RedNeuronalTorch\n",
    "class RedNeuronalTorch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RedNeuronalTorch, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 6)\n",
    "        self.fc2 = nn.Linear(6, 8)\n",
    "        self.fc3 = nn.Linear(8, 10)\n",
    "        self.fc4 = nn.Linear(10, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)  # No aplicar softmax: CrossEntropyLoss lo incluye internamente\n",
    "        return x\n",
    "\n",
    "# 3. Instanciar modelo, función de pérdida y optimizador\n",
    "modelo = RedNeuronalTorch()\n",
    "criterio = nn.CrossEntropyLoss()\n",
    "optimizador = optim.SGD(modelo.parameters(), lr=0.1)\n",
    "\n",
    "# 4. Entrenamiento\n",
    "epochs = 200\n",
    "loss_hist = []\n",
    "acc_hist = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    modelo.train()\n",
    "    outputs = modelo(X_train_t)\n",
    "    loss = criterio(outputs, y_train_t)\n",
    "\n",
    "    optimizador.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizador.step()\n",
    "\n",
    "    # Evaluación\n",
    "    modelo.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = modelo(X_train_t).argmax(dim=1)\n",
    "        acc = (preds == y_train_t).float().mean().item()\n",
    "    \n",
    "    loss_hist.append(loss.item())\n",
    "    acc_hist.append(acc)\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}, Accuracy = {acc:.4f}\")\n",
    "\n",
    "# 5. Graficar resultados\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_hist, label=\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Pérdida durante entrenamiento\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(acc_hist, label=\"Accuracy\", color=\"green\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Precisión durante entrenamiento\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "ocultar"
    ]
   },
   "outputs": [],
   "source": [
    "# Esta celda se ocultará\n",
    "\n",
    "from graphviz import Digraph, Graph\n",
    "\n",
    "def dibujar_red(red):\n",
    "    dot = Graph()\n",
    "    dot.attr(rankdir=\"LR\")\n",
    "    dot.attr(splines=\"false\")\n",
    "    dot.attr(nodesep=\"0.05\")\n",
    "    \n",
    "    for idx,capa in enumerate(red.arquitectura):\n",
    "        with dot.subgraph(name=f'cluster_{idx}') as c:\n",
    "            c.attr(rank=\"same\")\n",
    "            for i in range(capa['input_dim']+1):\n",
    "                c.node(nombre_nodo(idx, i), label=etiqueta_nodo(idx,i))\n",
    "\n",
    "            c.attr(color='white')\n",
    "            \n",
    "            label_extra = \"Entrada\" if idx == 0 else \"Oculta\\n(ReLU)\"\n",
    "        \n",
    "            c.attr(label=f'capa {idx+1}\\n{label_extra}')\n",
    "\n",
    "    with dot.subgraph(name=f'cluster_{idx+1}') as c:\n",
    "            c.attr(rank=\"same\")\n",
    "            for i in range(capa['output_dim']):\n",
    "                c.node(nombre_nodo(idx+1, i), label=etiqueta_nodo(idx+1,i, True))\n",
    "\n",
    "            c.attr(color='white')\n",
    "            \n",
    "            label_extra = \"Salida\\n(SoftMax)\"\n",
    "        \n",
    "            c.attr(label=f'capa {idx+1}\\n{label_extra}')\n",
    "\n",
    "    for idx, capa in enumerate(red.arquitectura):\n",
    "        for in_idx in range(capa[\"input_dim\"]+1):\n",
    "            for out_idx in range(capa[\"output_dim\"]):\n",
    "                to_node = (idx+1, out_idx+1) if idx!=len(red.arquitectura)-1 else (idx+1, out_idx)\n",
    "                dot.edge(nombre_nodo(idx, in_idx), \n",
    "                         nombre_nodo(*to_node))\n",
    "\n",
    "    return dot\n",
    "\n",
    "def nombre_nodo(capa, indice):\n",
    "    res = f\"c_{capa}_{indice}\"\n",
    "    return res\n",
    "\n",
    "def etiqueta_nodo(capa, indice, es_final=False):\n",
    "    if indice==0 and not es_final:\n",
    "        return \"1\"\n",
    "    l = \"a\" if capa!=0 else \"x\"\n",
    "    l = l if not es_final else \"y\"\n",
    "    \n",
    "    if l==\"x\" or l==\"y\":    \n",
    "        return f\"<{l}<sub>{indice}</sub>>\"\n",
    "    else:\n",
    "        return f\"<{l}<sub>{indice}</sub><sup>({capa})</sup>>\"\n",
    "\n",
    "# model = RedNeuronal()\n",
    "# model.add(...)\n",
    "# model._compile(...datos...)\n",
    "\n",
    "dibujar_red(model)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
