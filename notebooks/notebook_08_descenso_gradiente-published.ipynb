{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aprendizaje-automatico-dc-uba-ar/material/blob/main/notebooks/notebook_08_descenso_gradiente-published.ipynb)\n",
    "\n",
    "# Descenso por gradiente\n",
    "\n",
    "Vamos a explorar el método de descenso por gradiente para el cómputo de una regresión lineal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from typing import Callable, Dict, Optional, Any"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datos para trabajar\n",
    "\n",
    "Tenemos los siguientes datos generados al azar para ser ajustados por una regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = 500\n",
    "X, y, coef = make_regression(n_samples=5000, \n",
    "                             n_features=3, \n",
    "                             bias= bias,\n",
    "                             noise=1, random_state=42, coef=True)\n",
    "\n",
    "print(f\"Generamos una matriz de {X.shape[0]} de observaciones de {X.shape[1]} atributos\\n\" + \n",
    "      f\"Target en el rango {(round(min(y),2), round(max(y),2))}\\n\" + \n",
    "      f\"los coeficientes con los que fueron generados son: {coef}\")\n",
    "\n",
    "print(f\"Y = {bias} + {' + '.join([str(round(c,2)) + ' * X' + str(idx+1) for idx,c in enumerate(coef)])} + ε\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos se ven así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X, columns=[f\"X{i}\" for i in range(X.shape[1])])\n",
    "df[\"y\"] = y\n",
    "sns.pairplot(df.sample(50), hue=\"y\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook vamos a ver el resultado calculado de diversas formas, por lo que vamos a guardar para cada resultado los w y un valor de error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos esta métrica, pero podemos reemplazarla por otra\n",
    "\n",
    "def error_prediccion(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
    "    # agrego columna para el intercept\n",
    "    col_ones = np.ones((X.shape[0], 1))\n",
    "    X_ext = np.hstack((col_ones, X))\n",
    "    \n",
    "    predicciones = np.dot(X_ext, w)\n",
    "\n",
    "    return mean_squared_error(predicciones, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados los almacenaremos en un `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardo los datos para comparaciones posteriores\n",
    "resultados = pd.DataFrame(columns=[\"metodo\", \"w0\", \"w1\", \"w2\", \"w3\", \"error_prediccion\"])\n",
    "\n",
    "err = error_prediccion(X, y, np.array([bias] + list(coef)))\n",
    "resultados.loc[len(resultados)] = [\"simulacion\", bias] + list(coef) + [err]\n",
    "resultados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descenso por gradiente\n",
    "\n",
    "Veamos ahora la función general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descenso_gradiente(g, dg, z_init, alpha, num_iterations, tol, verbose=False):\n",
    "    \"\"\"\n",
    "    Descenso de gradiente para minimizar g. \n",
    "    Args:\n",
    "        g: La función a optimizar. \n",
    "        dg: El gradiente de la función.\n",
    "        z_init: Valor inicial.\n",
    "        alpha: El 'learning rate'.\n",
    "        num_iterations: Máx iteraciones.\n",
    "        tol: Tolerancia para la convergencia. \n",
    "    \"\"\"\n",
    "    z = z_init\n",
    "    for _ in range(num_iterations):\n",
    "        gradient = dg(z) # gradient vale por ej <0.5, -0.2, -3, 0>\n",
    "        z_new = z - (alpha * gradient)\n",
    "        if abs(g(z_new) - g(z)) < tol:\n",
    "            break\n",
    "        z = z_new\n",
    "    return z\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto es todo lo que necesitamos de descenso por gradiente."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión lineal\n",
    "\n",
    "Podemos generarnos una clase que calcula la regresión lineal utilizando el método de  descenso por gradiente y nos devuelva los pesos correspondientes a los w de la regresión.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from functools import partial\n",
    "\n",
    "class RegresionLinealDG():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 func_costo: Callable[..., float], \n",
    "                 func_costo_grad: Callable[..., np.ndarray], \n",
    "                 descenso_gradiente_hyperparams: Optional[Dict[str, Any]] = None):\n",
    "        self.func_costo = func_costo\n",
    "        self.func_costo_grad = func_costo_grad\n",
    "        self.descenso_gradiente_hyperparams = descenso_gradiente_hyperparams\n",
    "        self.collect_info = False\n",
    "   \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        # agrego columna para el intercept\n",
    "        col_ones = np.ones((X.shape[0], 1))\n",
    "        X_train_ext = np.hstack((col_ones, X))\n",
    "\n",
    "        cost_X_y = partial(self.func_costo, X=X_train_ext, y=y)\n",
    "        grad_cost_X_y = partial(self.func_costo_grad, X=X_train_ext, y=y)       \n",
    "                    \n",
    "        w_shape = X_train_ext.shape[1]\n",
    "        self.w = descenso_gradiente(cost_X_y, grad_cost_X_y, \n",
    "                        z_init=np.zeros(w_shape), \n",
    "                        **self.descenso_gradiente_hyperparams)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la clase `RegresionLinealDG` ya debemos definir en la creación la función de costo y el gradiente que se utilizan en el método (descenso por grandiente).\n",
    "\n",
    "Empecemos usando MSE:\n",
    "\n",
    "$MSE_{X,y} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{h}_{w_0,w_1,\\ldots,w_p}(x^{(i)}) - y^{(i)})^2$\n",
    "\n",
    "y su gradiente:\n",
    "\n",
    "$\\nabla_{\\mathbf{w}} MSE_{X,y}(w) = \\frac{2}{n} \\sum_{i=1}^{n} (\\hat{h}_{w_0,w_1,\\ldots,w_p}(x^{(i)}) - y^{(i)}) * x^{(i)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(w: np.ndarray, X: np.ndarray, y: np.ndarray) -> float:\n",
    "    y_preds = X @ w\n",
    "    return (1/len(y)) * np.sum((y_preds - y)**2)\n",
    " \n",
    "def grad_mse(w: np.ndarray, X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    y_preds = X @ w\n",
    "    return (2 / len(y)) * X.T @ (y_preds - y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos entonces crear nuestra regresión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparametros = {\"alpha\": 0.01, \"num_iterations\": 1000, \"tol\": 0.01}\n",
    "\n",
    "reg = RegresionLinealDG(mse, grad_mse, hyperparametros)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenarla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.fit(X,y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y obtener los w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"(DescensoG) w\", reg.w.round(1))\n",
    "\n",
    "err = error_prediccion(X, y, reg.w)\n",
    "resultados.loc[len(resultados)] = [f\"DescensoGradiente {hyperparametros['num_iterations']}\"] + list(reg.w) + [err]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solución analítica\n",
    "\n",
    "Analogamente, podemos computar el valor analitico de los _w_ con la fórmula:\n",
    "\n",
    "$w = (X^T X)^{-1} X^T y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "def minimizacion_analitica(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    col_ones = np.ones((X.shape[0], 1))\n",
    "    X_ext = np.hstack((col_ones, X))\n",
    "    X_XT_inv = inv(X_ext.T @ X_ext)\n",
    "    w_analitico = (X_XT_inv @ X_ext.T) @ y\n",
    "\n",
    "    return w_analitico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculemos los w:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_analitico = minimizacion_analitica(X, y)\n",
    "print(\"(Analítico) w\", w_analitico.round(1))\n",
    "\n",
    "err = error_prediccion(X, y, w_analitico)\n",
    "resultados.loc[len(resultados)] = [f\"Analítico\"] + list(w_analitico) + [err]\n",
    "resultados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios\n",
    "\n",
    "1. Realizar una grilla para los hyperparámetros de la regresión:\n",
    "\n",
    "    `{\"alpha\": 0.01, \"num_iterations\": 1000, \"tol\": 0.01}` \n",
    "    \n",
    "    y reportar su comportamiento.\n",
    "\n",
    "1. Implementar y experimentar el comportamiento con otra función de costo en lugar de MSE, elegir al menos una entre:\n",
    "    - MSE_Ridge\n",
    "    - MSE_Lasso\n",
    "    - MSE_ElasticSearch\n",
    "    \n",
    "   Reportar los resultados en el DataFrame.\n",
    "1. Modificar `RegresionLinealDG` para que realice mini-batch y verificar el funcionamiento comparando con el punto 1.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
