{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aprendizaje-automatico-dc-uba-ar/material/blob/main/notebooks/notebook_06_metricas-published.ipynb)\n", "\n", "# Evaluaci\u00f3n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%matplotlib inline\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import pandas as pd\n", "from IPython.display import display\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Matrices de confusi\u00f3n\n", "\n", "Recordemos que una matriz de confusi\u00f3n nos permite observar el resultado de nuestra clasificaci\u00f3n. Para armarla:\n", "- cada fila los valores observados o reales\n", "- cada columna los valores predichos\n", "\n", "Y en cada celda:\n", "- $m_{i,i}$ las instancias bien clasificadas\n", "- $m_{i,j}\\ (con\\ i\\neq j)$ las instancias mal clasificadas (era de instancia $i$ pero el clasificador dijo $j$)\n", "\n", "Implementar la siguiente funci\u00f3n para poder construir una matriz de confusi\u00f3n binaria. Deber\u00e1 tomar la etiqueta que es considerada \"\u00e9xito\" como par\u00e1metro."]}, {"cell_type": "code", "metadata": {}, "source": ["from typing import Tuple, Any\n", "\n", "def confusion_matrix(y_real: list, y_predicted: list, positive_label: Any, show: bool =False) -> Tuple[int, int, int, int]:\n", "    # Construye una matriz de confusi\u00f3n (binaria)\n", "    # y_actual es la secuencia de etiquetas reales\n", "    # y_predicted es la secuencia de etiquetas predichas por el clasificador\n", "    # positive_label indica cu\u00e1l es la etiqueta considerada positiva.\n", "\n", "    tp = \"COMPLETAR\"  # verdaderos positivos\n", "    tn = \"COMPLETAR\"  # verdaderos negativos\n", "    fp = \"COMPLETAR\"  # falsos positivos\n", "    fn = \"COMPLETAR\"  # falsos negativos\n", "    \n", "    if show:\n", "        display(pd.DataFrame([[tp, fn], [fp, tn]], index=[\"real +\", \"real -\"], columns=[\"pred +\", \"pred -\"]))\n", "        \n", "        \n", "    return tp, tn, fp, fn\n"], "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["### Test 1"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Vamos a probar ahora la funci\u00f3n. Sabiendo que se recibieron 990 emails. Y que se recibieron, en este orden:\n", "  - 10 correos no deseados\n", "  - 978 correos \n", "  - 2 correos no deseados\n", "  \n", "El filtro anti-spam estableci\u00f3 las siguientes clasificaciones (tambi\u00e9n en \u00f3rden):\n", "  - 2 correos no deseados\n", "  - 900 correos\n", "  - 20 correos no deseados\n", "  - 68 correos\n", "  \n", "Construir dos listas de strings que contengan `\"spam\"` o `\"no-spam\"` y que representen la etiqueta real (`y_real`) y la etiqueta predicha por el filtro anti-spam (`y_pred`)."]}, {"cell_type": "code", "metadata": {}, "source": ["y_real = \"...Completar...\"\n", "y_pred = \"...Completar...\""], "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["Correr la matriz de confusi\u00f3n y verificar que el resultado es el esperado."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tp, tn, fp, fn = confusion_matrix(y_real=y_real, y_predicted=y_pred, positive_label=\"spam\", show=False)\n", "# Si se cambia show a True se puede visualizar la matriz de confusi\u00f3n\n", "\n", "print(\"Test 1\")\n", "print(\"(tp, tn, fp, fn) = \", (tp, tn, fp, fn))\n", "assert((tp, tn, fp, fn) == (2, 958, 20, 10))\n", "print(\"OK\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## M\u00e9tricas\n", "\n", "En esta secci\u00f3n trabajeremos con las m\u00e9tricas est\u00e1ndares de clasificaci\u00f3n."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Test 2\n", "A continuacion completar las funciones que computan las distintas m\u00e9tricas"]}, {"cell_type": "code", "metadata": {}, "source": ["def accuracy_score(tp: int, tn: int, fp: int, fn: int) -> float:\n", "    return \"...Completar...\"\n", "\n", "\n", "def precision_score(tp: int, tn: int, fp: int, fn: int) -> float:\n", "    return \"...Completar...\"\n", "\n", "\n", "def recall_score(tp: int, tn: int, fp: int, fn: int) -> float:\n", "    return \"...Completar...\"\n", "\n", "\n", "def f_beta_score(tp: int, tn: int, fp: int, fn: int, beta: float) -> float:\n", "    prec = precision_score(tp, tn, fp, fn)\n", "    recl = recall_score(tp, tn, fp, fn)\n", "    return \"...Completar...\"\n", "\n", "\n", "def f1_score(tp: int, tn: int, fp: int, fn: int) -> float:\n", "    return f_beta_score(tp, tn, fp, fn, beta=1)\n", "\n", "\n", "def all_metrics(tp: int, tn: int, fp: int, fn: int) -> float:\n", "    accuracy = round(accuracy_score(tp, tn, fp, fn), 3)\n", "    precision = round(precision_score(tp, tn, fp, fn), 3)\n", "    recall = round(recall_score(tp, tn, fp, fn), 3)\n", "    f1 = round(f1_score(tp, tn, fp, fn), 3)\n", "    return accuracy, precision, recall, f1\n"], "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": ["Evaluar las funciones con el siguiente caso de test."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tp, tn, fp, fn = confusion_matrix(y_real=y_real, y_predicted=y_pred, positive_label=\"spam\")\n", "(accuracy, precision, recall, f1) = all_metrics(tp, tn, fp, fn)\n", "\n", "print(\"Test 2\")\n", "print(\"(accuracy, precision, recall, f1) = \", (accuracy, precision, recall, f1))\n", "assert((accuracy, precision, recall, f1) == (0.97, 0.091, 0.167, 0.118))\n", "print(\"OK\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Comparando predicciones\n", "\n", "Sean los siguientes datos provenientes de 2 clasificadores (A y B) y el valor real de las etiquetas."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Etiquetas reales\n", "y_real = [\"perro\"] * 18 + [\"gato\"] * 980 + [\"perro\"] * 5\n", "\n", "# Etiquetas devueltas por \"clasificador A\"\n", "y_pred_A = [\"gato\"] * 980 + [\"perro\"] * 20 + [\"gato\"] * 3\n", "\n", "# Etiquetas devueltas por \"clasificador B\"\n", "y_pred_B = [\"perro\"] * 40 + [\"gato\"] * 900 + [\"perro\"] * 60 + [\"gato\"] * 3\n", "\n", "df = pd.DataFrame(data={\"y_real\": y_real,\n", "                           \"y_pred_A\": y_pred_A,\n", "                           \"y_pred_B\": y_pred_B,                           \n", "                          })\n", "\n", "res = []\n", "print(\"Clasificador A, etiqueta de \u00e9xito: gato\")\n", "tp, tn, fp, fn = confusion_matrix(y_real=y_real, y_predicted=y_pred_A, positive_label=\"gato\", show=True)\n", "res.append(all_metrics(tp, tn, fp, fn))\n", "\n", "print(\"Clasificador B, etiqueta de \u00e9xito: gato\")\n", "tp, tn, fp, fn = confusion_matrix(y_real=y_real, y_predicted=y_pred_B, positive_label=\"gato\", show=True)\n", "res.append(all_metrics(tp, tn, fp, fn))\n", "\n", "print(\"Clasificador A, etiqueta de \u00e9xito: perro\")\n", "tp, tn, fp, fn = confusion_matrix(y_real=y_real, y_predicted=y_pred_A, positive_label=\"perro\", show=True)\n", "res.append(all_metrics(tp, tn, fp, fn))\n", "\n", "print(\"Clasificador B, etiqueta de \u00e9xito: perro\")\n", "tp, tn, fp, fn = confusion_matrix(y_real=y_real, y_predicted=y_pred_B, positive_label=\"perro\", show=True)\n", "res.append(all_metrics(tp, tn, fp, fn))\n", "\n", "pd.DataFrame(res, columns=[\"accuracy\", \"precision\", \"recall\", \"f1\"], index=[\"CLF A (gato)\", \"CLF B (gato)\", \"CLF A (perro)\", \"CLF B (gato)\"])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u00bfQu\u00e9 podemos concluir con este experimento?\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Analizando $F_1$\n", "\n", "A continuaci\u00f3n realizamos un experimento variando levemente las condiciones en cada pasada.\n", "\n", "El c\u00f3digo que realiza el experimento es el siguiente:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_real = [\"perro\"] * 100 + [\"gato\"] * 900 + [\"perro\"] * 80\n", "y_pred =   [\"perro\"] * 80 + [\"gato\"] * 800 + [\"perro\"] * 200\n", "\n", "tns_gato = []\n", "f1s_gato = []\n", "f1s_perro = []\n", "f1s_avg = []\n", "\n", "\n", "for i in range(0, 10000, 100):\n", "    y_real_2 = y_real + [\"perro\"] * i\n", "    y_pred_2 = y_pred + [\"perro\"] * i\n", "\n", "    tp1, tn1, fp1, fn1 = confusion_matrix(y_real=y_real_2, y_predicted=y_pred_2, positive_label=\"gato\")\n", "    tp2, tn2, fp2, fn2 = confusion_matrix(y_real=y_real_2, y_predicted=y_pred_2, positive_label=\"perro\")\n", "\n", "    f1_gato = f1_score(tp1, tn1, fp1, fn1)\n", "    f1_perro = f1_score(tp2, tn2, fp2, fn2)\n", "    f1_avg = (f1_gato + f1_perro) / 2\n", "\n", "    tns_gato.append(tn1)\n", "    f1s_gato.append(f1_gato)\n", "    f1s_perro.append(f1_perro)\n", "    f1s_avg.append(f1_avg)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["1. \u00bfQu\u00e9 realiza este experimento?\n", "1. \u00bfQu\u00e9 relaci\u00f3n existe entre la $F_1$ de perro y de gato a medida que se aumenta la cantidad de perros que tiene la muestra?\n", "1. \u00bfEn alg\u00fan punto valen lo mismo?\u00bfEn cu\u00e1l?\u00bfPor qu\u00e9?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["A continuaci\u00f3n se propone graficar c\u00f3mo var\u00eda la m\u00e9trica $F_1$ al aumentar la cantidad de True Negatives (observar\n", "que estamos cambiando la cantidad de instancias sobre las que testeamos). \n", "\n", "1. \u00bfQu\u00e9 curva modifica m\u00e1s el agregado de las etiquetas `perro`?\u00bfPor qu\u00e9? \n", "1. \u00bfQu\u00e9 se puede concluir de este experimento?\n", "    "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(15, 10))\n", "\n", "plt.plot(tns_gato, f1s_gato, \"*-\", label=\"F1_gato\")\n", "plt.plot(tns_gato, f1s_perro, \"o-\", label=\"F1_perro\")\n", "plt.plot(tns_gato, f1s_avg, \"x-\", label=\"F1_avg\")\n", "plt.xlabel(\"True Negatives (Gato)\")\n", "plt.ylabel(\"F1 score\")\n", "plt.ylim([0.5,1])\n", "plt.legend()\n", "plt.show()\n"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": ".venv", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.12"}}, "nbformat": 4, "nbformat_minor": 2}