{"cells": [{"cell_type": "markdown", "id": "06af5e67", "metadata": {}, "source": ["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aprendizaje-automatico-dc-uba-ar/material/blob/main/notebooks/notebook_11_cnn-published.ipynb)\n", "\n", "# Redes Convolucionales"]}, {"cell_type": "markdown", "id": "843b7bac", "metadata": {}, "source": ["En este notebook vamos a utilizar capas convoluciones para clasificar im\u00e1genes.\n", "\n", "Compararemos el desempe\u00f1o de una red densa con otra convolucional para clasificar im\u00e1genes de la base de datos [``EMNIST Letters``](https://www.nist.gov/itl/products-and-services/emnist-dataset). Un *dataset* de letras manuscritas, con 145.600 im\u00e1genes de 28x28 p\u00edxeles en escala de grises, distribuidas en 26 clases (una por cada letra del alfabeto ingl\u00e9s)."]}, {"cell_type": "code", "execution_count": null, "id": "b98e24de", "metadata": {}, "outputs": [], "source": ["import torch\n", "import torchvision\n", "import torchvision.transforms as transforms\n", "from sklearn.model_selection import train_test_split\n", "import torch.utils.data\n", "import numpy as np"]}, {"cell_type": "code", "execution_count": null, "id": "e337775b", "metadata": {}, "outputs": [], "source": ["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"]}, {"cell_type": "code", "execution_count": null, "id": "661391a4", "metadata": {}, "outputs": [], "source": ["torch.manual_seed(100)\n", "if torch.cuda.is_available():\n", "    torch.cuda.manual_seed(100)"]}, {"cell_type": "markdown", "id": "f5d1153c", "metadata": {}, "source": ["Separamos los conjuntos que vamos a usar para entrenar y evaluar"]}, {"cell_type": "code", "execution_count": null, "id": "ca19e3ba", "metadata": {}, "outputs": [], "source": ["train_transform = transforms.Compose(\n", "    [transforms.ToTensor(),\n", "     transforms.Normalize(0, 1)])\n", "\n", "val_transform = transforms.Compose(\n", "    [transforms.ToTensor(),\n", "     transforms.Normalize(0, 1)])"]}, {"cell_type": "code", "execution_count": null, "id": "4c5959e1", "metadata": {}, "outputs": [], "source": ["batch_size = 32\n", "\n", "train_set = torchvision.datasets.EMNIST(root = './data', split = 'letters', train = True, download = True, transform = train_transform)\n", "test_set = torchvision.datasets.EMNIST(root = './data', split = 'letters', train = False, download = True, transform = val_transform)\n", "targets_ = train_set.targets\n", "\n", "train_idx, val_idx = train_test_split(np.arange(len(targets_)), test_size = 0.2, stratify = targets_)\n", "train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n", "val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n", "\n", "trainloader = torch.utils.data.DataLoader(train_set, sampler = train_sampler, batch_size = batch_size, num_workers = 2)\n", "valloader = torch.utils.data.DataLoader(train_set, sampler = val_sampler, batch_size = batch_size, num_workers = 2)\n", "testloader = torch.utils.data.DataLoader(test_set, batch_size = batch_size, shuffle = False, num_workers = 2)"]}, {"cell_type": "code", "execution_count": null, "id": "bb05c836", "metadata": {}, "outputs": [], "source": ["print('Train set size:', len(train_set))\n", "print('Test set size:', len(test_set))"]}, {"cell_type": "markdown", "id": "0c8bfc3e", "metadata": {}, "source": ["# Exploraci\u00f3n de im\u00e1genes"]}, {"cell_type": "code", "execution_count": null, "id": "744bc592", "metadata": {}, "outputs": [], "source": ["%matplotlib inline\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "code", "execution_count": null, "id": "a0311722", "metadata": {}, "outputs": [], "source": ["def imshow(img):\n", "    img = img / 2 + 0.5 # Desnormalizamos.\n", "    npimg = img.numpy()\n", "    npimg = np.transpose(npimg, (1, 2, 0))\n", "    npimg = np.rot90(npimg, 3)\n", "    npimg = np.fliplr(npimg)\n", "    plt.imshow(npimg)\n", "    plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "761ee0bd", "metadata": {}, "outputs": [], "source": ["dataiter = iter(trainloader)\n", "images, labels = next(dataiter)\n", "\n", "imshow(torchvision.utils.make_grid(images))\n", "print(' '.join(f'{train_set.classes[labels[j]]:5s}' for j in range(batch_size)))"]}, {"cell_type": "markdown", "id": "7a03bbd0", "metadata": {}, "source": ["# Clasificamos im\u00e1genes (Red densa vs convolucional)"]}, {"cell_type": "markdown", "id": "57062117", "metadata": {}, "source": ["Ahora, armemos una red densa para la clasificaci\u00f3n de im\u00e1genes"]}, {"cell_type": "code", "execution_count": null, "id": "e59ac646", "metadata": {}, "outputs": [], "source": ["import torch.nn as nn\n", "import torch.nn.functional as F\n", "import torch.optim as optim"]}, {"cell_type": "code", "execution_count": null, "id": "49018030", "metadata": {}, "outputs": [], "source": ["class DenseNet(nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.fc1 = nn.Linear(28 * 28, 512)\n", "        self.fc2 = nn.Linear(512, 256)\n", "        self.fc3 = nn.Linear(256, 128)\n", "        self.fc4 = nn.Linear(128, 26)\n", "\n", "    def forward(self, x):\n", "        x = torch.flatten(x, 1)\n", "        x = F.leaky_relu(self.fc1(x))\n", "        x = F.leaky_relu(self.fc2(x))\n", "        x = F.leaky_relu(self.fc3(x))\n", "        x = self.fc4(x)\n", "        return x"]}, {"cell_type": "code", "execution_count": null, "id": "2b035102", "metadata": {}, "outputs": [], "source": ["net = DenseNet()\n", "net.to(device)"]}, {"cell_type": "markdown", "id": "a3fff717", "metadata": {}, "source": ["Entrenamos y evaluamos "]}, {"cell_type": "code", "execution_count": null, "id": "0799c8c5", "metadata": {}, "outputs": [], "source": ["learning_rate = 0.01\n", "momentum = 0.9\n", "\n", "criterion = nn.CrossEntropyLoss()\n", "optimizer = optim.SGD(net.parameters(), lr = learning_rate, momentum = momentum)"]}, {"cell_type": "code", "execution_count": null, "id": "a756662f", "metadata": {}, "outputs": [], "source": ["epochs = 2\n", "\n", "for epoch in range(epochs):\n", "\n", "    running_loss = 0.0\n", "    train_correct = 0\n", "    total = 0\n", "    for i, data in enumerate(trainloader, 0):\n", "        inputs, labels = data[0].to(device), data[1].to(device) - 1\n", "        # Restamos 1 para que las clases vayan de 0 a 25, en vez de 1 a 26.\n", "\n", "        optimizer.zero_grad() # Reiniciamos los gradientes.\n", "\n", "        outputs = net(inputs) # Hacemos la pasada forward.\n", "        loss = criterion(outputs, labels) # Calculamos la p\u00e9rdida.\n", "        loss.backward() # Hacemos la pasada backward.\n", "        optimizer.step() # Actualizamos los pesos.\n", "\n", "        running_loss += loss.item() * inputs.shape[0]\n", "\n", "        _, predicted = torch.max(outputs.data, 1)\n", "        total += labels.size(0)\n", "        train_correct += (predicted == labels).sum().item()\n", "\n", "    train_accuracy = 100 * train_correct / total\n", "    running_loss = running_loss / total\n", "\n", "    # Veamos en validaci\u00f3n.\n", "    val_correct = 0\n", "    total = 0\n", "    val_loss = 0\n", "    with torch.no_grad(): # Esta parte es s\u00f3lo validaci\u00f3n, no requiere entrenar, por lo que no calculamos gradientes.\n", "        for data in valloader:\n", "            images, labels = data[0].to(device), data[1].to(device) - 1\n", "            outputs = net(images)\n", "            _, predicted = torch.max(outputs.data, 1) # La ganadora es la que tiene mayor valor.\n", "            total += labels.size(0)\n", "            val_correct += (predicted == labels).sum().item()\n", "            val_loss += criterion(outputs, labels).item() * images.shape[0]\n", "\n", "    val_accuracy = 100 * val_correct / total\n", "    val_loss = val_loss / total\n", "\n", "    print(f'Epoch {epoch + 1 } finished. Train accuracy: {train_accuracy:.2f}%. Validation accuracy: {val_accuracy:.2f}%.')\n", "\n", "print('Finished training.')"]}, {"cell_type": "markdown", "id": "bd1bd41f", "metadata": {}, "source": ["Evaluamos en el conjunto de test"]}, {"cell_type": "code", "execution_count": null, "id": "76debf32", "metadata": {}, "outputs": [], "source": ["# Calculamos el accuracy para todo el conjunto de test.\n", "correct = 0\n", "total = 0\n", "\n", "first_predicted = None\n", "first_labels = None\n", "first_images = None\n", "\n", "with torch.no_grad(): # No necesitamos calcular gradientes.\n", "    for data in testloader:\n", "        images, labels = data[0].to(device), data[1].to(device) - 1 # Recordar restar 1 a las etiquetas.\n", "        outputs = net(images)\n", "        _, predicted = torch.max(outputs, 1)\n", "        if first_predicted is None:\n", "            first_predicted = predicted\n", "            first_labels = labels\n", "            first_images = images\n", "        total += labels.size(0)\n", "        correct += (predicted == labels).sum().item()\n", "\n", "accuracy = 100 * correct / total\n", "\n", "print(f'Test accuracy: {accuracy:.2f}%.')\n", "\n", "print('Predicted: ', ' '.join(f'{test_set.classes[first_predicted[j] + 1]:5s}' for j in range(5)))\n", "print('Ground truth: ', ' '.join(f'{test_set.classes[first_labels[j] + 1]:5s}' for j in range(5)))\n", "\n", "# Lo vemos s\u00f3lo para las primeras 5 im\u00e1genes.\n", "imshow(torchvision.utils.make_grid(first_images[:5].cpu()))"]}, {"cell_type": "markdown", "id": "09ec22fc", "metadata": {}, "source": ["## Probemos una red convolucional"]}, {"cell_type": "markdown", "id": "cc290de9", "metadata": {}, "source": ["Reemplazar la red densa por una red convolucional que aproveche la estructura espacial de la imagen, **sin aplanarla desde el inicio**. La red debe cumplir con los siguientes requisitos:\n", "\n", "- Tener **dos capas convolucionales**:\n", "  - La **primera** debe recibir im\u00e1genes de **1 canal** y producir **32 canales**.\n", "  - La **segunda** debe recibir los **32 canales** y producir **64 canales**.\n", "  - Ambas deben usar un **kernel de 3x3**, **stride 1** y **padding 1**.\n", "\n", "- Despu\u00e9s de **cada capa convolucional**, debe aplicarse:\n", "  - Una funci\u00f3n de activaci\u00f3n **LeakyReLU**.\n", "  - Una operaci\u00f3n de **max pooling** de tama\u00f1o **2x2** y **stride 2**.\n", "\n", "- Luego de las convoluciones y el aplanamiento:\n", "  - Agregar una **capa lineal** que reciba la salida de las convoluciones (de tama\u00f1o **64\u00d77\u00d77**) y la proyecte a **128 unidades**.\n", "  - Usar nuevamente una **activaci\u00f3n LeakyReLU**.\n", "  - Finalmente, agregar una **capa lineal de salida** con **26 unidades**, una por cada clase del conjunto **EMNIST Letters**.\n", "\n", "> \u270f\ufe0f **Recordar calcular bien el tama\u00f1o de entrada para la capa lineal seg\u00fan el efecto del pooling sobre las dimensiones espaciales.**\n"]}, {"cell_type": "code", "execution_count": null, "id": "4c569839", "metadata": {}, "outputs": [], "source": ["class CNN(nn.Module):\n", "    # Completar"]}, {"cell_type": "code", "execution_count": null, "id": "d7085335", "metadata": {}, "outputs": [], "source": ["net = CNN()\n", "net.to(device)"]}, {"cell_type": "markdown", "id": "c1b5a147", "metadata": {}, "source": ["Entrenamos y evaluamos como lo hicimos con la red densa"]}, {"cell_type": "code", "execution_count": null, "id": "b6adf108", "metadata": {}, "outputs": [], "source": ["learning_rate = 0.01\n", "momentum = 0.9\n", "\n", "criterion = nn.CrossEntropyLoss()\n", "optimizer = optim.SGD(net.parameters(), lr = learning_rate, momentum = momentum)\n", "\n", "epochs = 2\n", "\n", "for epoch in range(epochs):\n", "\n", "    running_loss = 0.0\n", "    train_correct = 0\n", "    total = 0\n", "    for i, data in enumerate(trainloader, 0):\n", "        inputs, labels = data[0].to(device), data[1].to(device) - 1\n", "\n", "        optimizer.zero_grad()\n", "\n", "        outputs = net(inputs)\n", "        loss = criterion(outputs, labels)\n", "        loss.backward()\n", "        optimizer.step()\n", "\n", "        running_loss += loss.item() * inputs.shape[0]\n", "\n", "        _, predicted = torch.max(outputs.data, 1)\n", "        total += labels.size(0)\n", "        train_correct += (predicted == labels).sum().item()\n", "\n", "    train_accuracy = 100 * train_correct / total\n", "    running_loss = running_loss/total\n", "\n", "    val_correct = 0\n", "    total = 0\n", "    val_loss = 0\n", "    with torch.no_grad():\n", "        for data in valloader:\n", "            images, labels = data[0].to(device), data[1].to(device) - 1\n", "            outputs = net(images)\n", "            _, predicted = torch.max(outputs.data, 1)\n", "            total += labels.size(0)\n", "            val_correct += (predicted == labels).sum().item()\n", "            val_loss += criterion(outputs, labels).item() * inputs.shape[0]\n", "\n", "    val_accuracy = 100 * val_correct / total\n", "    val_loss = val_loss / total\n", "\n", "    print(f'Epoch {epoch + 1} finished. Train accuracy: {train_accuracy:.2f}%. Validation accuracy: {val_accuracy:.2f}%.')\n", "\n", "print('Finished training.')"]}, {"cell_type": "markdown", "id": "914b978d", "metadata": {}, "source": ["Evaluamos en el conjunto de test"]}, {"cell_type": "code", "execution_count": null, "id": "b36cccd0", "metadata": {}, "outputs": [], "source": ["correct = 0\n", "total = 0\n", "\n", "first_predicted = None\n", "first_labels = None\n", "first_images = None\n", "\n", "with torch.no_grad():\n", "    for data in testloader:\n", "        images, labels = data[0].to(device), data[1].to(device) - 1\n", "        outputs = net(images)\n", "        _, predicted = torch.max(outputs, 1)\n", "        if first_predicted is None:\n", "            first_predicted = predicted\n", "            first_labels = labels\n", "            first_images = images\n", "        total += labels.size(0)\n", "        correct += (predicted == labels).sum().item()\n", "\n", "accuracy = 100 * correct / total\n", "\n", "print(f'Test accuracy: {accuracy:.2f}%.')\n", "\n", "print('Predicted: ', ' '.join(f'{test_set.classes[first_predicted[j] + 1]:5s}' for j in range(5)))\n", "print('Ground truth: ', ' '.join(f'{test_set.classes[first_labels[j] + 1]:5s}' for j in range(5)))\n", "\n", "imshow(torchvision.utils.make_grid(first_images[:5].cpu()))"]}, {"cell_type": "markdown", "id": "056f015a", "metadata": {}, "source": ["**Pregunta:** \u00bfCu\u00e1l tuvo mejor performance?"]}, {"cell_type": "markdown", "id": "7fda2e99", "metadata": {}, "source": ["# \u00bfPodemos interpretar a la red convolucional?"]}, {"cell_type": "markdown", "id": "8f9f6fa7", "metadata": {}, "source": ["En esta parte queremos entender qu\u00e9 est\u00e1 haciendo la red, para eso vamos a visualizar los filtros y c\u00f3mo se aplican a la imagen. "]}, {"cell_type": "code", "execution_count": null, "id": "d4f3548c", "metadata": {}, "outputs": [], "source": ["from torchvision import utils\n", "\n", "# ch = canal que queremos ver\n", "\n", "def visTensor(tensor, ch=0, allkernels=False, nrow=8, padding=1, normalize = False):\n", "  n,c,w,h = tensor.shape\n", "\n", "  if allkernels: tensor = tensor.view(n*c, -1, w, h) # -1 dice \"no s\u00e9 cu\u00e1l es el valor de este par\u00e1metro, calculalo\"\n", "  elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)\n", "\n", "  tensor = tensor[:32] # no mostramos m\u00e1s de 32 filtros\n", "\n", "  rows = np.min((tensor.shape[0] // nrow + 1, 16))\n", "  grid = utils.make_grid(tensor, nrow=nrow, normalize = normalize, padding=padding)\n", "  plt.figure( figsize=(nrow,rows) )\n", "  plt.imshow(grid.numpy().transpose((1, 2, 0)))"]}, {"cell_type": "markdown", "id": "54ef7a98", "metadata": {}, "source": ["Visualizamos los filtros de la primera capa (9x9) para el canal 0 (estamos en escala de grises)"]}, {"cell_type": "code", "execution_count": null, "id": "1b33d5a6", "metadata": {}, "outputs": [], "source": ["filter = net.conv1.weight.data.clone()\n", "visTensor(filter.cpu(), ch=0, allkernels=True, normalize = False)\n", "\n", "plt.axis('off')\n", "plt.ioff()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "8c2143cc", "metadata": {}, "source": ["Tambi\u00e9n podemos visualizamor los filtros de la segunda capa (3x3)\n"]}, {"cell_type": "code", "execution_count": null, "id": "dacee988", "metadata": {}, "outputs": [], "source": ["filter = net.conv2.weight.data.clone()\n", "visTensor(filter.cpu(), normalize = True, allkernels=False)\n", "\n", "plt.axis('off')\n", "plt.ioff()\n", "plt.show()"]}, {"cell_type": "markdown", "id": "72d8a4c4", "metadata": {}, "source": ["Los filtros no son demasiado claros. Pero, podemos ver c\u00f3mo se activan si pasamos una imagen. Por ejemplo, podemos elegir la siguiente imagen:"]}, {"cell_type": "code", "execution_count": null, "id": "b4797b1c", "metadata": {}, "outputs": [], "source": ["plt.imshow(first_images[1].cpu().squeeze(), cmap='viridis')\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "bd371755", "metadata": {}, "outputs": [], "source": ["# Pass image through the first convolutional layer\n", "conv_layers = [net.conv1, net.maxPool1, net.activation1, net.conv2, net.maxPool2]\n", "conv_out = [net.conv1(first_images[1])]\n", "# Iteratively pass image through all convolutional layers\n", "for i in range(1, len(conv_layers)):\n", "  conv_out.append(conv_layers[i](conv_out[-1]))"]}, {"cell_type": "code", "execution_count": null, "id": "35999959", "metadata": {}, "outputs": [], "source": ["conv_out[0].shape"]}, {"cell_type": "code", "execution_count": null, "id": "fae5d03d", "metadata": {}, "outputs": [], "source": ["def imshow_grid(img, ax, cmap = 'gray', vmin = None):\n", "    img = img / 2 + 0.5 # Desnormalizamos.\n", "    npimg = img.numpy()\n", "    npimg = np.transpose(npimg, (1, 2, 0))\n", "    npimg = np.rot90(npimg, 3)\n", "    npimg = np.fliplr(npimg)\n", "    ax.imshow(npimg, cmap = cmap, vmin = vmin)"]}, {"cell_type": "markdown", "id": "e06a63bf", "metadata": {}, "source": ["Vemos c\u00f3mo se activa el primer filtro de las redes convolucionales. Es decir, visualizamos el primer mapa de activaci\u00f3n resultante de la primera capa"]}, {"cell_type": "code", "execution_count": null, "id": "8cca9440", "metadata": {}, "outputs": [], "source": ["plt.imshow(conv_out[0][0].detach().cpu().squeeze(), cmap='viridis')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "8fe6867d", "metadata": {}, "source": ["O c\u00f3mo se activa el segundo filtro de las capas convolucionales, es decir, el segundo mapa de activaci\u00f3n resultante de la primera capa convolucional:"]}, {"cell_type": "code", "execution_count": null, "id": "613a2592", "metadata": {}, "outputs": [], "source": ["plt.imshow(conv_out[0][1].detach().cpu().squeeze(), cmap='viridis')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "ddef31bf", "metadata": {}, "source": ["Podemos ver c\u00f3mo se activan los primeros 16 filtros de la primer capa convolucional:"]}, {"cell_type": "code", "execution_count": null, "id": "f49ae1a1", "metadata": {}, "outputs": [], "source": ["fig, axs = plt.subplots(4, 4, figsize=(4, 4))\n", "fig.gca().set_axis_off()\n", "fig.gca().xaxis.set_major_locator(plt.NullLocator())\n", "fig.gca().yaxis.set_major_locator(plt.NullLocator())\n", "axs = axs.flatten()\n", "for i in range(0, 16):\n", "  imshow_grid(conv_out[0][i].detach().cpu().unsqueeze(0), axs[i], cmap='viridis')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "d947a38b", "metadata": {}, "source": ["**Ejercicio**: Visualizar qu\u00e9 pasa despu\u00e9s del primer max pool"]}], "metadata": {"kernelspec": {"display_name": ".venv", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.12"}}, "nbformat": 4, "nbformat_minor": 5}