{"cells": [{"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aprendizaje-automatico-dc-uba-ar/material/blob/main/notebooks/notebook_07_clustering-published.ipynb)\n", "\n", "# Clustering\n", "\n", "En este notebook implementaremos algunos de los conceptos de _clustering_ vistos en clase.\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from scipy.cluster.hierarchy import dendrogram\n", "from matplotlib import pyplot as plt\n", "\n", "plt.rcParams['figure.figsize'] = [10, 5] # para ver los gr\u00e1ficos m\u00e1s grandes"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Probando K-means y DBSCAN"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%matplotlib inline \n", "\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from sklearn import cluster, datasets\n", "from sklearn.preprocessing import StandardScaler\n", "\n", "# crear y normalizar dataset\n", "N = 1500\n", "X, y = datasets.make_blobs(n_samples=N, centers=3, cluster_std=1.0,random_state=2211)\n", "X = StandardScaler().fit_transform(X)\n", "\n", "# ejecutar k-means con k=2\n", "algorithm = cluster.MiniBatchKMeans(n_clusters=2)\n", "algorithm.fit(X)\n", "y_pred = algorithm.labels_.astype(int)\n", "\n", "# graficar\n", "colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])\n", "colors = np.hstack([colors] * 20)\n", "plt.scatter(X[:, 0], X[:, 1], color=colors[y_pred].tolist(), s=10)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Ejercicio** \n", "1. Encontrar clusters sobre el mismo dataset utilizando la implementacion de sklearn de [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html).\n", "2. Experimentar con los hiperpar\u00e1metros de ambos modelos."]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Implementaciones de algoritmos\n", "\n", "En esta secci\u00f3n implementar\u00e1n K-Means, Clustering Aglomerativo, DBScan y GMMs."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## K-Means\n", "\n", "Recordemos el algoritmo de KMeans:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["```\n", "Hiperpar\u00e1metros: K\n", "\n", "1. Ubicar K centroides al azar.\n", "2. Iterar hasta cumplir con alg\u00fan criterio de convergencia:\n", "\n", "   a. Computar distancias (euclidianas) entre cada punto a cada uno de los centroides.\n", "\n", "   b. A cada punto, asignar cluster seg\u00fan cercan\u00eda a cada centroide.\n", "   \n", "   c. Mover centroides a la media de cada cluster.\n", "\n", "El centroide de un cluster es el vector promedio de las instancias pertenecientes al cluster (vector con promedios de cada atributo).\n", "```"]}, {"cell_type": "markdown", "metadata": {}, "source": ["1. Definir la funci\u00f3n que inicializa los k centroides al azar: `inicializacion_centroides`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def inicializacion_centroides(puntos: np.ndarray, k: int) -> np.ndarray:\n", "    \"\"\"Inicializa los k centroides eligiendo de manera random k de\n", "      los puntos\"\"\"\n", "    #COMPLETARS\n", "    return centroides\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["2. Completar la funci\u00f3n `centroides_cercanos` que calcula, para cada punto, el centroide m\u00e1s cercano que ser\u00e1 adem\u00e1s donde tendremos a qu\u00e9 cluster pertenece cada uno de los puntos."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def centroides_cercanos(puntos: np.ndarray, centroides: np.ndarray) -> np.ndarray:\n", "    \"\"\" Devuelve un array que contiene el indice al centroide m\u00e1s cercano para cada punto.\"\"\"\n", "    #COMPLETAR\n", "    return distancias"]}, {"cell_type": "markdown", "metadata": {}, "source": ["3. Completar la funci\u00f3n `mover_centroides` que mueve a los centroides al punto medio de cada cluster y devuelve los nuevos centroides."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def mover_centroides(puntos: np.ndarray, cercanos: np.ndarray, k: int) -> np.ndarray:\n", "    \"\"\" Mueve los centroides a la media de cada cluster.\"\"\"\n", "    #COMPLETAR\n", "    return nuevos_centroides"]}, {"cell_type": "markdown", "metadata": {}, "source": ["4. Probar la implementaci\u00f3n de las funciones"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def kmeans(puntos: np.ndarray, k: int, max_iters: int = 100) -> np.ndarray:\n", "    \"\"\" Algoritmo K-means. \"\"\"\n", "    centroides = inicializacion_centroides(puntos, k)\n", "    for _ in range(max_iters):\n", "        nuevos_clusters = centroides_cercanos(puntos, centroides)\n", "        centroides = mover_centroides(puntos, nuevos_clusters, k)\n", "        \n", "    return centroides, nuevos_clusters"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["np.random.seed(42)\n", "puntos = np.random.rand(100, 2)  #100 puntos en dimension 2\n", "k = 3  \n", "centroids, closest = kmeans(puntos, k)\n", "\n", "plt.scatter(puntos[:, 0], puntos[:, 1], c=closest, s=50, cmap='viridis')\n", "plt.scatter(centroids[:, 0], centroids[:, 1], color='red', s=200, alpha=0.5)\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Clustering aglomerativo"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ahora compleatremos un algoritmo de clustering jer\u00e1rquico que utiliza una estrategia _bottom-up_.\n", "\n", "### Pseudo-c\u00f3digo\n", "\n", "```\n", "1. Cada punto es un cluster: C1 = x(1); \u2026 ; Cn = x(n) \n", "2. Calcular una matriz de distancias entre todo par de clusters. \n", "3. Buscar en la matriz de distancia el par de clusters m\u00e1s similares. \n", "4. Mergear 2 clusters: A y B seg\u00fan su distancia. \n", "    a. Borrar las filas de la matriz de distancias correspondientes a los clusters A y B. \n", "    b. Agregar una nueva fila que contenga la distancia entre A \u222a B y el resto de los clusters. \n", "5. Repetir los pasos 3 y 4 hasta conseguir la cantidad de clusters deseados (o hasta que haya s\u00f3lo un cluster). \n", "```\n", "\n", "El objetivo ser\u00e1 tener una funci\u00f3n:\n", "\n", "```python\n", "def aglomerativo(datos, num_clusters=1, criterio_distancia=\"min\")\n", "```\n", "\n", "Para ello se propone:\n", "\n", "1. Implementar la funci\u00f3n `inicializar_distancias(datos)` que toma una matriz de numpy, donde cada fila es una instancia y las columnas son los features y devuelve una matriz de distancias. Las matriz debe ser triangular inferior para calcular las distancias, los valores en y sobre la diagonal deben ser `NaN`.\n", "En este caso usaremos la distancia Euclediana."]}, {"cell_type": "code", "metadata": {"vscode": {"languageId": "raw"}}, "source": ["def inicializar_distancias(datos: np.ndarray) -> np.ndarray:\n", "    \n", "    #COMPLETAR"], "outputs": [], "execution_count": null}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Tests:\n", "X_0 = np.array([[1], [2], [5], [-3]])\n", "y_0_resp = inicializar_distancias(X_0)\n", "y_0_exp = np.array([[np.nan, np.nan, np.nan, np.nan], [1., np.nan, np.nan, np.nan], [ 4., 3., np.nan, np.nan], [4.,  5., 8., np.nan]])\n", "np.testing.assert_allclose(y_0_resp, y_0_exp, rtol=1e-5)\n", "print(\"Pas\u00e9 test 0\")\n", "\n", "\n", "X_1 = np.array([[1,2], [2,3], [3,5]])\n", "y_1_resp = inicializar_distancias(X_1)\n", "y_1_exp = np.array([[np.nan, np.nan, np.nan], [1.41421356, np.nan, np.nan], [3.60555128, 2.23606798, np.nan]])\n", "np.testing.assert_allclose(y_1_resp, y_1_exp, rtol=1e-5)\n", "print(\"Pas\u00e9 test 1\")\n", "\n", "\n", "X_2 = np.array([[5.1, 3.5, 1.4, 0.2], [4.9, 3. , 1.4, 0.2], [4.7, 3.2, 1.3, 0.2], [4.6, 3.1, 1.5, 0.2], [5. , 3.6, 1.4, 0.2]])\n", "y_2_resp = inicializar_distancias(X_2)\n", "y_2_exp = np.array([[np.nan, np.nan, np.nan, np.nan, np.nan],  [0.53851648, np.nan, np.nan, np.nan, np.nan],  [0.50990195, 0.3, np.nan, np.nan, np.nan],  [0.64807407, 0.33166248, 0.24494897, np.nan, np.nan],  [0.14142136, 0.60827625, 0.50990195, 0.64807407, np.nan]])\n", "np.testing.assert_allclose(y_2_resp, y_2_exp, rtol=1e-5)\n", "print(\"Pas\u00e9 test 2\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["2. Definir la funci\u00f3n `encontrar minimo(distancias)` que toma una matriz de distancias como la que se gener\u00f3 en el punto anterior y devuelve la fila y la columna de su menor valor."]}, {"cell_type": "code", "metadata": {"vscode": {"languageId": "raw"}}, "source": ["from typing import Tuple\n", "\n", "def encontrar_minimo(distancias: np.ndarray) -> Tuple[int, int]:\n", "    \n", "    #COMPLETAR\n", "    \n", "    return (2, 1)"], "outputs": [], "execution_count": null}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Tests:\n", "\n", "d_0 = np.array([[np.nan, np.nan, np.nan, np.nan], [1., np.nan, np.nan, np.nan], [ 4., 3., np.nan, np.nan], [4.,  5., 8., np.nan]])\n", "pos_0_resp = encontrar_minimo(d_0)\n", "pos_0_exp = (1,0)\n", "np.testing.assert_allclose(pos_0_resp, pos_0_exp, rtol=1e-5)\n", "print(\"Pas\u00e9 test 0\")\n", "\n", "\n", "d_1 = np.array([[np.nan, np.nan, np.nan], [1.41421356, np.nan, np.nan], [3.60555128, 2.23606798, np.nan]])\n", "pos_1_resp = encontrar_minimo(d_1)\n", "pos_1_exp = (1,0)\n", "np.testing.assert_allclose(pos_1_resp, pos_1_exp, rtol=1e-5)\n", "print(\"Pas\u00e9 test 1\")\n", "\n", "\n", "d_2 = np.array([[np.nan, np.nan, np.nan, np.nan, np.nan],  [0.53851648, np.nan, np.nan, np.nan, np.nan],  [0.50990195, 0.3, np.nan, np.nan, np.nan],  [0.64807407, 0.33166248, 0.24494897, np.nan, np.nan],  [0.14142136, 0.60827625, 0.50990195, 0.64807407, np.nan]])\n", "pos_2_resp = encontrar_minimo(d_2)\n", "pos_2_exp = (4,0)\n", "np.testing.assert_allclose(pos_2_resp, pos_2_exp, rtol=1e-5)\n", "print(\"Pas\u00e9 test 2\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["3. Definir la funci\u00f3n `actualizar_distancias(distancias, i, j, criterio=\"min\")` que toma una matriz de distancias y recalcula la distancia de un nuevo cluster que sea la uni\u00f3n del que tiene \u00edndice $i$ con el de \u00edndice $j$, que saque las columnas $i$ y $j$ y agregue una nueva columna/fila al final con las distancias de este nuevo cluster armado con los dem\u00e1s. Debe al menos tener el criterio de distancias entre clusters \"m\u00ednimo\" u \"m\u00e1ximo\", opcionalmente se puede completar con \"vinculaci\u00f3n promedio\"."]}, {"cell_type": "code", "metadata": {"vscode": {"languageId": "raw"}}, "source": ["def actualizar_distancias(d: np.ndarray, i: int, j: int, criterio: str = \"min\") -> np.ndarray:\n", "    \n", "    #COMPLETAR\n", "    \n", "    return distancias"], "outputs": [], "execution_count": null}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Tests:\n", "\n", "d_0 = np.array([[np.nan, np.nan, np.nan, np.nan], [1., np.nan, np.nan, np.nan], [ 4., 3., np.nan, np.nan], [4.,  5., 8., np.nan]])\n", "pos_0_resp = actualizar_distancias(d_1,1,0)\n", "pos_0_exp = np.array([[np.nan, np.nan], [2.23606798, np.nan]])\n", "np.testing.assert_allclose(pos_0_resp, pos_0_exp, rtol=1e-5)\n", "print(\"Pas\u00e9 test 0\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["4. Revisar el c\u00f3digo de `aglomerativo` y correrlo para el ejemplo\n", "\n", "    ```python\n", "        X = np.array([[i] for i in [2, 8, 0, 4, 1, 9, 9, 0]])\n", "\n", "        cl, u = aglomerativo(X, criterio=\"max\")\n", "\n", "        fig = plt.figure(figsize=(10, 6),facecolor='white')\n", "        dn = dendrogram(u)\n", "    ```\n", "\n", "    Verificar que obtienen un resultado equivalente al mostrado como ejemplo de [esta p\u00e1gina](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def aglomerativo(datos, num_clusters=1, criterio_distancia=\"min\"):\n", "    distancias = inicializar_distancias(datos)\n", "\n", "    uniones = []\n", "    clusters = [[idx] for idx,_ in enumerate(datos)]\n", "    nombres_de_clusters = [idx for idx,_ in enumerate(datos)]\n", "    prox_cluster = len(nombres_de_clusters) \n", "\n", "    while distancias.shape[0] > num_clusters:\n", "        # Buscar el par de clusters m\u00e1s similares\n", "        i, j = encontrar_minimo(distancias)\n", "        d = distancias[i][j]\n", "        \n", "        # Fusionar los clusters A y B\n", "        nuevo_cluster = clusters[i]\n", "        nuevo_cluster.extend(clusters[j])\n", "        clusters.append(nuevo_cluster)\n", "\n", "        # agrego la uni\u00f3n que realizo\n", "        uniones.append([nombres_de_clusters[i], nombres_de_clusters[j], d, len(nuevo_cluster)])\n", "\n", "        # agrego la composici\u00f3n del nuevo cluster\n", "        nombres_de_clusters.append(prox_cluster)\n", "        prox_cluster+=1\n", "\n", "        # Actualizar la matriz de distancias\n", "        distancias = actualizar_distancias(distancias, i, j, criterio=criterio_distancia)\n", "\n", "        # Sacar la info de los clusters sin fusionar\n", "        clusters = [c for idx,c in enumerate(clusters) if idx not in [i,j]]\n", "        nombres_de_clusters = [n for idx,n in enumerate(nombres_de_clusters) if idx not in [i,j]]\n", "\n", "\n", "    return clusters, np.array(uniones)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["5. Crear un dendograma con el dataset de Iris y verificar si la clusterizaci\u00f3n generada es o no consistente con lo expresado en los targets\n", "\n", "    ```python\n", "        # Cargamos el dataset que usaremos hoy\n", "        from sklearn.datasets import load_iris\n", "        iris_dataset = load_iris()\n", "\n", "        X = pd.DataFrame(iris_dataset.data)\n", "        y = pd.Series(iris_dataset.target)\n", "    ```\n", "\n", "    Explorar las siguientes condiciones:\n", "\n", "    1. Criterio de distancias\n", "\n", "    1. \u00bfSe obtiene una mejor, peor o similar clusterizaci\u00f3n al normalizar los atributos?\n", " "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## DBScan"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Algoritmo** Hiper*HiperPar\u00e1metros*: ***m (min_samples)***, ***\u03b5 (epsilon)***. \n", "\n", "1. *Etiquetar a todo punto:*\n", "   1. como **Core** si tiene al menos ***m*** vecinos a menos de ***\u03b5*** distancia.\n", "   2. **Non-Core** si no.\n", "2. Conectamos a **cada core point con sus vecinos** formando un grafo.\n", "3. A partir de cada punto **Core** escanear a qu\u00e9 puntos puede llegar a trav\u00e9s de saltos en este grafo. \n", "4. Clasificamos a los **Non-Core** como:\n", "   1. **Border**: si qued\u00f3 conectado en el grafo (es decir, si ten\u00eda al menos un core en su vecindario y no es core).\n", "   2. **Noise**: al resto.\n", "5. Cada componente conexa se corresponde a un cluster. \n", "6. No asignamos ning\u00fan cluster a los puntos **Noise**."]}, {"cell_type": "markdown", "metadata": {}, "source": ["1. Se tiene una lista con los vecinos m\u00e1s cercanos a cada punto dada por \u00b4encontrar_vecinos\u00b4. Implementar la funci\u00f3n encontrar_puntos_core que devuelve un array con el tipo de punto para cada uno."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.neighbors import NearestNeighbors\n", "from typing import List\n", "\n", "def encontrar_vecinos(X: np.ndarray, epsilon: float) -> List[List[int]]:\n", "    # Implementaci\u00f3n del Paso 1: Encontrar vecinos a menos de epsilon distancia\n", "    neigh = NearestNeighbors(radius=epsilon)\n", "    neigh.fit(X)\n", "    return neigh.radius_neighbors(X, epsilon, return_distance=False)\n", "\n", "def etiquetar_puntos_core(vecinos: List[List[int]], min_samples: int) -> np.ndarray:\n", "    # Implementaci\u00f3n del Paso 1.1 y 1.2: Etiquetar puntos como Core o Non-Core\n", "    return puntos"]}, {"cell_type": "markdown", "metadata": {}, "source": ["2. Implementaci\u00f3n del paso 3: Expandir cluster a partir de un punto Core."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def expandir_cluster(idx: int, vecinos: List[List[int]], puntos: np.ndarray, \n", "                     cluster_labels: np.ndarray, cluster_id: int) -> np.ndarray:\n", "    # Implementaci\u00f3n del Paso 3: Expandir cluster a partir de un punto Core\n", "    # TODO: Implementar la expansi\u00f3n del cluster\n", "    pass\n", "\n", "def formar_clusters(puntos: np.ndarray, vecinos: List[List[int]]) -> np.ndarray:\n", "    # Implementaci\u00f3n del Paso 2 y 3: Conectar core points y formar clusters\n", "    n_points = len(puntos)\n", "    cluster_labels = np.full(n_points, -1, dtype=int)\n", "    cluster_id = 0\n", "    for idx, label in enumerate(puntos):\n", "        if label == 'Core' and cluster_labels[idx] == -1:\n", "            cluster_labels[idx] = cluster_id\n", "            cluster_labels = expandir_cluster(idx, vecinos, puntos, cluster_labels, cluster_id)\n", "            cluster_id += 1\n", "    return cluster_labels"]}, {"cell_type": "markdown", "metadata": {}, "source": ["3. A cada punto Non-Core clasificarlo como Border o Noise"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def clasificar_non_core(puntos, vecinos):\n", "    # Implementaci\u00f3n del Paso 4: Clasificar puntos Non-Core como Border o Noise\n", "    # TODO: Implementar la clasificaci\u00f3n de puntos Non-Core\n", "    pass"]}, {"cell_type": "markdown", "metadata": {}, "source": ["4. Definici\u00f3n del algoritmo. Revisar el c\u00f3digo."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def dbscan(X, min_samples, epsilon):\n", "    # Paso 1\n", "    vecinos = encontrar_vecinos(X, epsilon)\n", "    point_labels = etiquetar_puntos_core(vecinos, min_samples)\n", "    \n", "    # Pasos 2 y 3\n", "    cluster_labels = formar_clusters(point_labels, vecinos)\n", "    \n", "    # Paso 4\n", "    point_labels = clasificar_non_core(point_labels, vecinos)\n", "    \n", "    # Pasos 5 y 6 se implementan impl\u00edcitamente en la formaci\u00f3n de clusters\n", "    return cluster_labels, point_labels"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##  EM para GMMs"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Inicializaci\u00f3n:\n", "Decidir cuantos clusters (c) utilizaremos. Luego, inicializar los par\u00e1metros para $c$ normales: $c$ medias ($\\mu_c$), $c$  desvios ($\\sigma_c$), y $c$ pesos ($\\pi_c$).\n", "\n", "### Paso E:\n", "\n", "Calcular para cada punto $x^{(i)}$ la probabilidad ($r_{ic}$) de que el punto pertenezca al cluster c:\n", "$$r_{ic} = P(c | x^{(i)}) = \\frac{P(c)P(x^{(i)} | c)}{P(x^{(i)})} = \\frac{\\pi_c N(x^{(i)} \\ | \\ \\mu_c,\\sigma_c)}{\\Sigma_{k=1}^K \\pi_k N(x^{(i)} \\ | \\ \\mu_k,\\sigma_k)}$$\n", "\n", "donde \n", "\n", "$${\\displaystyle {\\begin{aligned}N(x^{(i)},\\mu_c,\\sigma_c)&{}={\\frac {1}{\\sigma_c {\\sqrt {2\\pi }}}}e^{-{\\frac {(x^{(i)}-\\mu_c )^{2}}{2\\sigma_c ^{2}}}},\\quad \\pi = 3.141592... .\\\\\\end{aligned}}}$$\n", "\n", "### Paso M:\n", "Para cada cluster c, actualizar $\\pi_c$, $\\mu_c$, y $\\sigma_c$ seg\u00fan los datos:\n", "\n", "$$ N_c = \\sum_{i}{r_{ic}}$$\n", "\n", "$$ \\mu^{(nuevo)}_c = \\frac{1}{N_c}\\sum_{i}{r_{ic}x^{(i)}}$$\n", "$$ \\sigma^{(nuevo)}_c = \\frac{1}{N_c}\\sum_{i}{r_{ic}(x^{(i)} - \\mu^{(nuevo)}_c)^2} $$\n", "$$ \\pi^{(nuevo)}_c = \\frac{N_c}{n}$$\n", "\n", "\n", "\n", "### Iterar \n", "Iterar hasta que la log-likelihood del modelo converja:\n", "\n", "$$ln \\ p(\\boldsymbol{X} \\ | \\ \\boldsymbol{\\pi},\\boldsymbol{\\mu},\\boldsymbol{\\sigma}) \\ = \\ \\Sigma_{i=1}^N \\ ln(\\Sigma_{k=1}^K \\pi_k N(x^{(i)} \\ | \\ \\mu_k,\\sigma_k))$$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%matplotlib inline\n", "import scipy.stats\n", "from sklearn.cluster import KMeans\n", "import math \n", "import seaborn as sns\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from sklearn.mixture import GaussianMixture\n", "\n", "def gmm_train(X, n_clusters, tolerance = 1e-3):\n", "    v_mu, v_sigma, v_pi = inicializacion(n_clusters, X)\n", "    old_llk = 0\n", "    tolerance = 1e-3 # indica la m\u00ednima ganancia en llk para que el algoritmo contin\u00fae.\n", "    gain = np.inf\n", "\n", "    while(gain > tolerance):\n", "        print_iter(v_mu, v_sigma, v_pi, n_clusters)\n", "        r = e(X, n_clusters, v_mu, v_sigma, v_pi)\n", "        v_mu, v_sigma, v_pi = m(X, n_clusters, r)\n", "        llk = log_likelihood(X, v_mu, v_sigma, v_pi, n_clusters)\n", "        print(\"Log likelihood: \", llk)\n", "        gain = llk - old_llk\n", "   \n", "    return v_mu, v_sigma, v_pi\n", "\n", "def inicializacion(n_clusters, X):\n", "    ##################\n", "    # COMPLETAR\n", "    # Inicializar v_mu, v_sigma, v_pi (vectores que contienen una componente por cluster)\n", "    ##################\n", "    \n", "    return v_mu, v_sigma, v_pi\n", "\n", "def norm(x, mu, sigma):\n", "    return scipy.stats.norm(mu, sigma).pdf(x)\n", "\n", "def e(X, n_clusters, v_mu, v_sigma, v_pi):\n", "    r = np.ones((len(X), n_clusters))\n", "    \n", "    ##################\n", "    # COMPLETAR\n", "    # r[i, c] debera contener la probabilidad estimada P(c | x^(i))\n", "    ##################\n", "    \n", "    return r\n", "\n", "def m(X, n_clusters, r):\n", "    N = np.zeros(n_clusters)\n", "    v_mu_new = np.zeros(n_clusters)\n", "    v_sigma_new = np.zeros(n_clusters)\n", "    v_pi_new = np.zeros(n_clusters)\n", "    \n", "    ##################\n", "    # COMPLETAR\n", "    # v_mu_new, v_sigma_new, v_pi_new deber\u00e1n contener los valores actualizados.\n", "    ##################\n", "    \n", "    return v_mu_new, v_sigma_new, v_pi_new\n", "\n", "\n", "def log_likelihood(X, v_mu, v_sigma, v_pi, n_clusters):\n", "    suma = 0\n", "    for i in range(len(X)):\n", "        suma_k = 0\n", "        for k in range(n_clusters):\n", "            suma_k += v_pi[k] * norm(X[i], v_mu[k], v_sigma[k])\n", "        suma += math.log(suma_k)\n", "    return suma\n", "\n", "\n", "def print_iter( v_mu, v_sigma, v_pi, n_clusters):\n", "    for c in range(n_clusters):\n", "        print(\"C{} => mu={} +- {} (pi={})\".format(c+1, \"%.2f\" % v_mu[c], \"%.2f\" % v_sigma[c], \"%.2f\" % v_pi[c]), end=\" | \")\n", "    print()\n", "\n", "def dibujar(X, v_mu, v_sigma, v_pi, n_clusters, title=\"GMMs\"):\n", "    plt.figure()\n", "    plt.title(title)\n", "    colors = sns.color_palette(\"hls\", n_clusters)\n", "    plt.plot(X, [1] * len(X), \".\")\n", "    for k in range(n_clusters):\n", "        col = colors[k]\n", "#         plt.plot([v_mu[k], v_mu[k]], [0, 2], label=\"$\\mu_{}$\".format(k), color=col)\n", "        plt.plot([v_mu[k] - v_sigma[k], v_mu[k] + v_sigma[k]], [1.1,1.1], color=col, label=\"$\\mu_{}$\".format(k))\n", "        plt.plot([v_mu[k] - v_sigma[k], v_mu[k] - v_sigma[k]], [0.9,1.2], color=col)\n", "        plt.plot([v_mu[k] + v_sigma[k], v_mu[k] + v_sigma[k]], [0.9,1.2], color=col)\n", "    plt.yticks([])\n", "    plt.ylim([0, 2])\n", "    plt.legend()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Datos para ejemplo\n", "np.random.seed(134)\n", "X = np.array([10, 11, 12, 20, 21, 22, 30, 31, 32, 29, 33])\n", "n_clusters = 3\n", "\n", "# DESCOMENTAR\n", "v_mu, v_sigma, v_pi = gmm_train(X, n_clusters)\n", "dibujar(X, v_mu, v_sigma, v_pi, n_clusters, title=\"implementacion propia\")\n", "#print(\"DESCOMENTAR lo anterior\")\n", "\n", "print(\"Salida esperada:\")\n", "# SALIDA ESPERADA:\n", "sklearn_gmm = GaussianMixture(n_components=3)\n", "sklearn_gmm.fit(X.reshape(-1, 1))\n", "\n", "dibujar(X, sklearn_gmm.means_.ravel(), sklearn_gmm.covariances_.ravel(), sklearn_gmm.weights_.ravel(), n_clusters, title=\"sklearn\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["-------------------------------------------------------------------"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": ".venv", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.12"}}, "nbformat": 4, "nbformat_minor": 2}