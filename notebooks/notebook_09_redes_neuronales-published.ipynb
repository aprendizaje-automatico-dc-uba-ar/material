{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aprendizaje-automatico-dc-uba-ar/material/blob/main/notebooks/notebook_09_redes_neuronales-published.ipynb)\n", "\n", "# Redes neuronales\n", "\n", "\n", "Vamos nuevamente a trabajar con los datos de `iris` para entrenar (y antes construir) una Red Neuronal."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import LabelEncoder\n", "from sklearn.datasets import load_iris\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "def get_data():\n", "    dataset = load_iris()\n", "    X = dataset[\"data\"]\n", "    y = dataset[\"target\"]\n", "    y = LabelEncoder().fit_transform(y)\n", "    return np.array(X), np.array(y)\n", "X, y = get_data()\n", "X"]}, {"cell_type": "markdown", "metadata": {}, "source": ["La propuesta es empezar por el esqueleto de las 2 clases que usaremos para esta tarea e ir implementado los m\u00e9todos a medida que avancemos.\n", "\n", "Al final de este notebook se encuentran ambas clases completas. Pueden copiar el c\u00f3digo desde all\u00ed mismo o implementarlo. La idea es que en cada avance podamos comprender la parte del proceso que estamos realizando, por lo cual se recomienda seguir la guia propuesta e ir completando s\u00f3lo lo que es necesario para cada punto."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Capa:\n", "    def __init__(self, neuronas):\n", "        self.neuronas = neuronas\n", "\n", "    def forward(self, inputs, weights, bias, activation):\n", "        \"\"\"\n", "        Forward Propagation de la capa\n", "        \"\"\"\n", "        raise NotImplementedError\n", "        \n", "    def relu(self, inputs):\n", "        \"\"\"\n", "        ReLU: funci\u00f3n de activaci\u00f3n\n", "        \"\"\"\n", "        raise NotImplementedError\n", "\n", "    def softmax(self, inputs):\n", "        \"\"\"\n", "        Softmax: funci\u00f3n de activaci\u00f3n\n", "        \"\"\"\n", "        raise NotImplementedError\n", "    \n", "    def backward(self, dA_curr, W_curr, Z_curr, A_prev, activation):\n", "        \"\"\"\n", "        Backward Propagation de la capa\n", "        \"\"\"\n", "        raise NotImplementedError\n", "\n", "    def relu_derivative(self, dA, Z):\n", "        \"\"\"\n", "        ReLU: gradiente de ReLU\n", "        \"\"\"\n", "        raise NotImplementedError"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class RedNeuronal:\n", "    def __init__(self, learning_rate=0.01):\n", "        self.red = [] ## capas\n", "        self.arquitectura = [] ## mapeo de entradas -> salidas\n", "        self.pesos = [] ## W, b\n", "        self.memoria = [] ## Z, A\n", "        self.gradientes = [] ## dW, db\n", "        self.lr = learning_rate\n", "        \n", "    def add(self, capa):\n", "        \"\"\"\n", "        Agregar capa a la red\n", "        \"\"\"\n", "        self.network.append(capa)\n", "            \n", "    def _compile(self, data):\n", "        \"\"\"\n", "        Inicializar la arquitectura\n", "        \"\"\"\n", "        raise NotImplementedError\n", "    \n", "    def _init_weights(self, data):\n", "        \"\"\"\n", "        Inicializar arquitectura y los pesos\n", "        \"\"\"\n", "        raise NotImplementedError\n", "    \n", "    def _forwardprop(self, data):\n", "        \"\"\"\n", "        Pasada forward completa por la red\n", "        \"\"\"\n", "        raise NotImplementedError\n", "    \n", "    def _backprop(self, predicted, actual):\n", "        \"\"\"\n", "        Pasada backward completa por la red\n", "        \"\"\"\n", "        raise NotImplementedError\n", "            \n", "    def _update(self):\n", "        \"\"\"\n", "        Actualizar el modelo --> lr * gradiente\n", "        \"\"\"\n", "        raise NotImplementedError\n", "    \n", "    def _get_accuracy(self, predicted, actual):\n", "        \"\"\"\n", "        Calcular accuracy despu\u00e9s de cada iteraci\u00f3n\n", "        \"\"\"\n", "        raise NotImplementedError\n", "    \n", "    def _calculate_loss(self, predicted, actual):\n", "        \"\"\"\n", "        Calcular cross-entropy loss despu\u00e9s de cada iteraci\u00f3n\n", "        \"\"\"\n", "        raise NotImplementedError\n", "    \n", "    def train(self, X_train, y_train, epochs):\n", "        \"\"\"\n", "        Entrenar el modelo Stochastic Gradient Descent\n", "        \"\"\"\n", "        raise NotImplementedError"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Los items que se presentan a continuaci\u00f3n tienen como objetivo explorar las clases que componen la red neuronal propuesta, comprender su arquitectura y funcionamiento.\n", "\n", "Nuevamente, lo ideal es no mirar todos los m\u00e9todos hasta que llegue el momento de utilizarlos. \n", "\n", "1. Crear una Red Neuronal con 6 nodos en la primera capa, 8 en la segunda, 10 en la tercer y finalmente 3 en la \u00faltima, utilizando los m\u00e9todos `add()`, `_compile()` de la clase `RedNeuronal` y el constructor de la clase `Capa`.\n", "  \n", "    Imprimir la arquitectura del modelo y asegurarse de obtener:\n", "\n", "    ```\n", "    [{'input_dim': 4, 'output_dim': 6, 'activation': 'relu'},\n", "    {'input_dim': 6, 'output_dim': 8, 'activation': 'relu'},\n", "    {'input_dim': 8, 'output_dim': 10, 'activation': 'relu'},\n", "    {'input_dim': 10, 'output_dim': 3, 'activation': 'softmax'}]\n", "    ```\n", "\n", "    Dibujar la red en papel.\n", "\n", "1. Inicializar los pesos de la red del punto anterior (`_init_weights(datos)`) y verificar que los pesos tienen dimensi\u00f3n correcta:\n", "\n", "    ```\n", "    capa 0: w=(4, 6) - b=(1, 6)\n", "    capa 1: w=(6, 8) - b=(1, 8)\n", "    capa 2: w=(8, 10) - b=(1, 10)\n", "    capa 3: w=(10, 3) - b=(1, 3)\n", "    ```\n", "\n", "    Definir las matrices que se corresponden con las capas de manera que una pasada pueda ser interpretada como el producto de todas ellas. Recordar que en cada paso por cada capa estaremos computando por cada neurona de la capa siguiente:\n", "\n", "    $$Z = \\sum_{i=1}^{n} X_i \\times W_i + b$$\n", "\n", "1. Funciones de activaci\u00f3n de una `Capa`:\n", "\n", "    1. Verificar que el funcionamiento de `ReLU` se corresponda con:\n", "\n", "        ```\n", "        if input > 0:\n", "            return input\n", "        else:\n", "            return 0\n", "        ``` \n", "\n", "    1. Verificar que el funcionamiento de `softmax` se corresponda con:\n", "\n", "        $$\\sigma(Z)_i = \\frac{e^{z_i}}{\\sum_{i=1}^{n} e^{z_j}}$$\n", "\n", "    **Nota**: para probar estos dos m\u00e9todos puede ser util construir un vector de la siguiente manera: `np.array([[1.3, 5.1, -2.2, 0.7, 1.1]])` que genera un vector de tama\u00f1o (1,5).\n", "\n", "1. Avancemos con `_forwardprop(datos)`, si corremos la red inicializada con los datos:\n", "\n", "    1. \u00bfQu\u00e9 nos tipo de objeto nos devuelve este m\u00e9todo?\n", "\n", "    1. \u00bfQu\u00e9 quiere decir cada uno de los valores?\n", "\n", "    1. La primera fila, que se corresponder\u00eda con la primera observaci\u00f3n del dataset, \u00bfqu\u00e9 resultados nos da?\u00bfqu\u00e9 es m\u00e1s probable: 'setosa', 'versicolor' o 'virginica'?\u00bfqu\u00e9 valor es el real?\u00bfpor qu\u00e9?\n", "\n", "1. Arrancamos a propagar para atr\u00e1s lo aprendido en la primera pasada. Esto lo realizaremos con el m\u00e9todo `_backprop`.\n", "\n", "    1. \u00bfC\u00f3mo es la derivada de la funci\u00f3n de activaci\u00f3n `ReLU`?\u00bfSu c\u00f3digo es correcto?\n", "\n", "    1. \u00bfCu\u00e1l es la operaci\u00f3n matem\u00e1tica que hace la funci\u00f3n `backward` de la clase `Capa` en el caso de tener como activaci\u00f3n a `relu`?\n", "\n", "    1. El m\u00e9todo `_backprop` toma 2 par\u00e1metros: `predicted` y `actual`. \u00bfqu\u00e9 debemos pasarle en dicho lugar?\n", "\n", "        Si la respuesta no fue: en `predicted` le pasamos el resultado de `_forwardprop(...)` y en `actual` le pasamos `y`.... volver a pensarlo. ;-)\n", "\n", "    1. Verificar que los `gradientes` y los `pesos` para cada una de las capas tienen el mismo tama\u00f1o.\n", "\n", "1. Preparemos por \u00faltimo las funciones necesarias para el entrenamiento. Describir brevemente qu\u00e9 hacen las funciones:\n", "\n", "    - `_get_accuracy`\n", "    - `_calculate_loss`\n", "    - `_update`\n", "\n", "1. Incluyamos finalmente la funci\u00f3n `train` y entrenemos una red con la arquitectura propuesta en el punto 1 por 200 epocas.\n", "\n", "    1. \u00bfQu\u00e9 valores se imprimen?\u00bfQu\u00e9 es posible interpretar de ellos?\n", "\n", "    1. Graficar el _accuracy_ y la _loss_ que arroja el entramiento en funci\u00f3n de las _epochs_. \u00bfQu\u00e9 se puede concluir? Probablemente la se\u00f1al sea ruidosa, por lo que se recomienda hacer un suavizado por ventanas deslizantes.\n", "\n", "1. Reimplementar la clase `RedNeuronal` utilizando PyTorch\n", "\n", "    Hasta ahora hemos construido nuestra propia red neuronal \"desde cero\", lo cual nos permiti\u00f3 comprender en profundidad c\u00f3mo funciona cada componente: inicializaci\u00f3n de pesos, funciones de activaci\u00f3n, forward y backward propagation, c\u00e1lculo de loss y accuracy, y actualizaci\u00f3n de pesos.\n", "\n", "    Sin embargo, en proyectos reales y m\u00e1s complejos, utilizamos frameworks como **PyTorch** que abstraen estas tareas, permiti\u00e9ndonos enfocarnos m\u00e1s en el dise\u00f1o de la arquitectura y el an\u00e1lisis de los resultados.  \n", "\n", "    **Objetivo de este inciso**: recrear la arquitectura y entrenamiento de nuestra red neuronal, pero usando herramientas provistas por PyTorch. Esto implica:\n", "\n", "    1. Implementar una clase `RedNeuronalTorch` que herede de `nn.Module` y contenga una red con la misma arquitectura:  \n", "    - Entrada de dimensi\u00f3n 4 (por las caracter\u00edsticas del dataset Iris)\n", "    - Capas ocultas de 6, 8 y 10 nodos respectivamente\n", "    - Capa de salida con 3 nodos y activaci\u00f3n `softmax`\n", "\n", "    2. Entrenar esta nueva red por 200 \u00e9pocas utilizando:\n", "    - Funci\u00f3n de p\u00e9rdida: `nn.CrossEntropyLoss`\n", "    - Optimizador: `torch.optim.SGD`\n", "    - Tasa de aprendizaje: 0.01\n", "\n", "    3. Comparar los resultados obtenidos con los del entrenamiento anterior (implementaci\u00f3n manual). Algunas preguntas a responder:\n", "    - \u00bfLa convergencia es m\u00e1s r\u00e1pida o m\u00e1s lenta?\n", "    - \u00bfC\u00f3mo se comporta la p\u00e9rdida durante el entrenamiento?\n", "    - \u00bfCu\u00e1l implementaci\u00f3n fue m\u00e1s f\u00e1cil de modificar o extender?\n", "\n", "    4. Graficar la evoluci\u00f3n de la **p\u00e9rdida** y el **accuracy** durante las \u00e9pocas para ambas implementaciones (manual y PyTorch), idealmente en la misma figura para facilitar la comparaci\u00f3n. Pod\u00e9s aplicar una media m\u00f3vil para suavizar la se\u00f1al.\n", "\n", "    > \ud83d\udca1 **Sugerencia pedag\u00f3gica**: antes de realizar este inciso, se recomienda repasar los notebooks `9a` y `9b`, donde se presentan una introducci\u00f3n a los tensores y al workflow de ML usando PyTorch.\n", "\n", "\n", "Cr\u00e9dito: este ejercicio se base en la propuesta de Joe Sasson publicada en [Towards Data Science](https://towardsdatascience.com/coding-a-neural-network-from-scratch-in-numpy-31f04e4d605)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### C\u00f3digo completo (Implementaci\u00f3n con Numpy)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class Capa:\n", "    def __init__(self, neuronas):\n", "        self.neuronas = neuronas\n", "\n", "    def forward(self, inputs, weights, bias, activation):\n", "        \"\"\"\n", "        Forward Propagation de la capa\n", "        \"\"\"\n", "        Z_curr = np.dot(inputs, weights.T) + bias\n", "\n", "        if activation == 'relu':\n", "            A_curr = self.relu(inputs=Z_curr)\n", "        elif activation == 'softmax':\n", "            A_curr = self.softmax(inputs=Z_curr)\n", "\n", "        return A_curr, Z_curr\n", "\n", "    def relu(self, inputs):\n", "        \"\"\"\n", "        ReLU: funci\u00f3n de activaci\u00f3n\n", "        \"\"\"\n", "\n", "        return np.maximum(0, inputs)\n", "\n", "    def softmax(self, inputs):\n", "        \"\"\"\n", "        Softmax: funci\u00f3n de activaci\u00f3n\n", "        \"\"\"\n", "        exp_scores = np.exp(inputs)\n", "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n", "        return probs\n", "         \n", "    def backward(self, dA_curr, W_curr, Z_curr, A_prev, activation):\n", "        \"\"\"\n", "        Backward Propagation de la capa\n", "        \"\"\"\n", "        if activation == 'softmax':\n", "            dW = np.dot(A_prev.T, dA_curr)\n", "            db = np.sum(dA_curr, axis=0, keepdims=True)\n", "            dA = np.dot(dA_curr, W_curr) \n", "        else:\n", "            dZ = self.relu_derivative(dA_curr, Z_curr)\n", "            dW = np.dot(A_prev.T, dZ)\n", "            db = np.sum(dZ, axis=0, keepdims=True)\n", "            dA = np.dot(dZ, W_curr)\n", "            \n", "        return dA, dW, db\n", "\n", "    def relu_derivative(self, dA, Z):\n", "        \"\"\"\n", "        ReLU: gradiente de ReLU\n", "        \"\"\"\n", "        dZ = np.array(dA, copy = True)\n", "        dZ[Z <= 0] = 0\n", "        return dZ\n", "    "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class RedNeuronal:\n", "    def __init__(self, learning_rate=0.01):\n", "        self.red = [] ## capas\n", "        self.arquitectura = [] ## mapeo de entradas -> salidas\n", "        self.pesos = [] ## W, b\n", "        self.memoria = [] ## Z, A\n", "        self.gradientes = [] ## dW, db\n", "        self.lr = learning_rate\n", "        \n", "    def add(self, capa):\n", "        \"\"\"\n", "        Agregar capa a la red\n", "        \"\"\"\n", "        self.red.append(capa)\n", "            \n", "    def _compile(self, data):\n", "        \"\"\"\n", "        Inicializar la arquitectura\n", "        \"\"\"\n", "        for idx, _ in enumerate(self.red):\n", "            if idx == 0:\n", "                self.arquitectura.append({'input_dim': data.shape[1], \n", "                                        'output_dim': self.red[idx].neuronas,\n", "                                        'activation':'relu'})\n", "            elif idx > 0 and idx < len(self.red)-1:\n", "                self.arquitectura.append({'input_dim': self.red[idx-1].neuronas, \n", "                                        'output_dim': self.red[idx].neuronas,\n", "                                        'activation':'relu'})\n", "            else:\n", "                self.arquitectura.append({'input_dim': self.red[idx-1].neuronas, \n", "                                        'output_dim': self.red[idx].neuronas,\n", "                                        'activation':'softmax'})\n", "        return self\n", "\n", "    def _init_weights(self, data):\n", "        \"\"\"\n", "        Inicializar arquitectura y los pesos\n", "        \"\"\"\n", "        self._compile(data)\n", "\n", "        np.random.seed(99)\n", "\n", "        for i in range(len(self.arquitectura)):\n", "            self.pesos.append({\n", "                'W':np.random.uniform(low=-1, high=1, \n", "                        size=(self.arquitectura[i]['input_dim'],\n", "                            self.arquitectura[i]['output_dim']\n", "                            )),\n", "                'b':np.zeros((1, self.arquitectura[i]['output_dim']))})\n", "\n", "        return self\n", "    \n", "    def _forwardprop(self, data):\n", "        \"\"\"\n", "        Pasada forward completa por la red\n", "        \"\"\"\n", "        A_curr = data\n", "\n", "        for i in range(len(self.pesos)):\n", "            A_prev = A_curr\n", "            A_curr, Z_curr = self.red[i].forward(inputs=A_prev, \n", "                                                    weights=self.pesos[i]['W'].T, \n", "                                                    bias=self.pesos[i]['b'], \n", "                                                    activation=self.arquitectura[i]['activation'])\n", "\n", "            self.memoria.append({'inputs':A_prev, 'Z':Z_curr})\n", "\n", "        return A_curr\n", "    \n", "    def _backprop(self, predicted, actual):\n", "        \"\"\"\n", "        Pasada backward completa por la red\n", "        \"\"\"\n", "        num_samples = len(actual)\n", "\n", "        ## compute the gradient on predictions\n", "        dscores = predicted\n", "        dscores[range(num_samples),actual] -= 1\n", "        dscores /= num_samples\n", "\n", "        dA_prev = dscores\n", "\n", "        for idx, layer in reversed(list(enumerate(self.red))):\n", "            dA_curr = dA_prev\n", "\n", "            A_prev = self.memoria[idx]['inputs']\n", "            Z_curr = self.memoria[idx]['Z']\n", "            W_curr = self.pesos[idx]['W']\n", "\n", "            activation = self.arquitectura[idx]['activation']\n", "\n", "            dA_prev, dW_curr, db_curr = layer.backward(dA_curr, W_curr.T, Z_curr, A_prev, activation)\n", "\n", "            self.gradientes.append({'dW':dW_curr, 'db':db_curr})\n", "\n", "        self.gradientes = list(reversed(self.gradientes))  # Reverse the gradients list\n", "\n", "    def _update(self):\n", "        \"\"\"\n", "        Actualizar el modelo --> lr * gradiente\n", "        \"\"\"\n", "        lr = self.lr\n", "        for idx, layer in enumerate(self.red):\n", "            self.pesos[idx]['W'] -= lr * self.gradientes[idx]['dW']\n", "            self.pesos[idx]['b'] -= lr * self.gradientes[idx]['db']\n", "\n", "    def _get_accuracy(self, predicted, actual):\n", "        \"\"\"\n", "        Calcular accuracy despu\u00e9s de cada iteraci\u00f3n\n", "        \"\"\"\n", "        return np.mean(np.argmax(predicted, axis=1)==actual)\n", "        \n", "    def _calculate_loss(self, predicted, actual):\n", "        \"\"\"\n", "        Calculate cross-entropy loss after each iteration\n", "        \"\"\"\n", "        samples = len(actual)\n", "\n", "        correct_logprobs = -np.log(predicted[range(samples),actual])\n", "        data_loss = np.sum(correct_logprobs)/samples\n", "\n", "        return data_loss\n", "\n", "    def train(self, X_train, y_train, epochs):\n", "        \"\"\"\n", "        Entrenar el modelo Stochastic Gradient Descent\n", "        \"\"\"\n", "        self.loss = []\n", "        self.accuracy = []\n", "\n", "        self._init_weights(X_train)\n", "\n", "        for i in range(epochs):\n", "            yhat = self._forwardprop(X_train)\n", "            self.accuracy.append(self._get_accuracy(predicted=yhat, actual=y_train))\n", "            self.loss.append(self._calculate_loss(predicted=yhat, actual=y_train))\n", "\n", "            self._backprop(predicted=yhat, actual=y_train)\n", "\n", "            self._update()\n", "\n", "            if i % 20 == 0:\n", "                s = 'EPOCH: {}, ACCURACY: {}, LOSS: {}'.format(i, self.accuracy[-1], self.loss[-1])\n", "                print(s)\n", "\n", "        return (self.accuracy, self.loss)\n"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": ".venv", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.12"}}, "nbformat": 4, "nbformat_minor": 2}